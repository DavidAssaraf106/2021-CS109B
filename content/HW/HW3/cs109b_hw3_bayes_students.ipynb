{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "## Homework 3: Bayesian Analysis\n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2021**<br/>\n",
    "**Instructors**: Mark Glickman, Pavlos Protopapas, and Chris Tanner \n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/\"\n",
    "    \"content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "- Please restart the kernel and run the entire notebook again before you submit.\n",
    "\n",
    "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. **Please use only the libraries provided in those imports.**\n",
    "\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` statement that clearly labels the output, includes a reference to the calculated value, and rounds it to a reasonable number of digits. **Do not hard code values in your printed output**. For example, this is an appropriate print statement:\n",
    "```python\n",
    "print(f'The R^2 is {R:.4f}')\n",
    "```\n",
    "- Your plots should be clearly labeled, including clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is NOT a descriptive title; \"95% confidence interval of coefficients of polynomial degree 5\" on the other hand is descriptive).\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\David\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyMC3 version: 3.9.3\n",
      "Using ArviZ version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "from scipy.special import expit\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Using PyMC3 version: {pm.__version__}\")\n",
    "print(f\"Using ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore a common pymc3 warning that comes from library functions, not our code.\n",
    "# Pymc3 may throw additional warnings, but other warnings should be manageable\n",
    "# by following the instructions included within the warning messages.\n",
    "import warnings\n",
    "\n",
    "messages=[\n",
    "    \"Using `from_pymc3` without the model will be deprecated in a future release\",\n",
    "]\n",
    "\n",
    "for m in messages:\n",
    "    warnings.filterwarnings(\"ignore\", message=m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook Contents\n",
    "\n",
    "- [**Part A: Rejection sampling and the weighted bootstrap**](#part1)\n",
    "  - [Overview](#part1intro)\n",
    "  - [Questions](#part1questions)\n",
    "  - [Solutions](#part1solutions)\n",
    "\n",
    "\n",
    "- [**Part B: Bayesian Logistic Regression and Varying-Intercepts Model**](#part2)\n",
    "  - [Overview](#part2intro)\n",
    "  - [Questions](#part2questions)\n",
    "  - [Solutions](#part2solutions)\n",
    "\n",
    "\n",
    "- [**Part C: Varying-Coefficients Model and Model Selection**](#part3)\n",
    "  - [Overview](#part3intro)\n",
    "  - [Questions](#part3questions)\n",
    "  - [Solutions](#part3solutions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contraceptive Usage by Bangladeshi Women\n",
    "\n",
    "For this assignment, you are provided with data sets `dataset_2_train.csv` and `dataset_2_test.csv`containing details of contraceptive usage by 1,934 Bangladeshi women.\n",
    "\n",
    "There are four attributes for each woman, along with a label `contraceptive_use` indicating if she uses contraceptives. The attributes include:\n",
    "\n",
    "* `district`: identifying code for the district the woman lives in\n",
    "* `urban`: type of region of residence\n",
    "* `living.children`: number of living children\n",
    "* `age-mean`: age of the woman (in years, centered around mean)\n",
    "\n",
    "The women are grouped into 60 districts. The task is to build a classification model that can predict if a given woman uses contraceptives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# Part A:  Rejection sampling and the weighted bootstrap\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1intro\"></a>\n",
    "\n",
    "## Overview \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "For the first part of the problem, we will only work with the label `contraceptive_use` and ignore all the attributes.  Let $Y_i$ be 1 if woman $i$ uses contraceptives, and 0 otherwise.  Assume a Bernoulli model for the data:\n",
    "\n",
    "$$Y_{i} \\sim \\text{Bernoulli}(\\theta)$$\n",
    "\n",
    "where $i=1,\\ldots,N$, with $N$ being the number of observations in the training data set, and $\\theta$ is the unknown probability a woman uses contraception.  We will assume the following prior distribution on $\\theta$:\n",
    "\n",
    "$$\\theta \\sim \\text{Normal}(0.5, 0.5^2)$$\n",
    "\n",
    "subject to $0 \\leq \\theta \\leq 1$.  This is sometimes called a truncated normal distribution.  A value from this distribution can be randomly drawn by simulating a value from $\\text{Normal}(0.5, 0.5^2)$ and then keeping it if the value is between 0 and 1, and trying again if it is outside this range.  In fact, this is a form of rejection sampling.  The density for the truncated normal distribution is\n",
    "\n",
    "$$p(\\theta) = c\\times\\frac{1}{\\sqrt{2\\pi (0.5)^2}} \\: \\exp\\left(\\frac{-1}{2(0.5)^2}(\\theta-0.5)^2\\right) \\; \\text{for} \\; 0\\leq \\theta \\leq 1 \\; \\text{, and} \\; 0 \\; \\text{otherwise,}$$\n",
    "\n",
    "where $c$ is a normalizing constant that does not depend on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1questions\"></a>\n",
    "\n",
    "### <div class='exercise'>Part A: Questions</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "<a id=\"qA1\"></a>\n",
    "\n",
    "**[A1](#sA1)** \n",
    "\n",
    "Given the training data, what is the likelihood function $L(\\theta | y_1,\\ldots,y_n)$?  What is the MLE of $\\theta$ as a function of the $y_1,\\ldots,y_n$?  Compute the MLE from the data.\n",
    "\n",
    "<a id=\"qA2\"></a>\n",
    "\n",
    "**[A2](#sA2)**\n",
    "\n",
    "Using rejection sampling, simulate a sample of 10,000 accepted values $\\theta$ from the posterior distribution.  Plot a histogram of these values, and provide numerical summaries of the distribution of the 10,000 values.  Interpret your findings.\n",
    "\n",
    "<a id=\"qA3\"></a>\n",
    "\n",
    "**[A3](#sA3)**\n",
    "\n",
    "Carry out the weighted bootstrap to simulate 1,000 values of $\\theta$ from the posterior distribution.  In doing so, simulate 10,000 values from the prior distribution to use as the discrete distribution from which the posterior draws will be simulated via the importance weights.  As above, plot a histogram of these values, and provide numerical summaries of the distribution of 1,000 values.  Interpret the results, and compare to the results of rejection sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1solutions\"></a>\n",
    "\n",
    "## Part A: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sA1\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[A1](#qA1)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Given the training data, what is the likelihood function $L(\\theta | y_1,\\ldots,y_n)$?  What is the MLE of $\\theta$ as a function of the $y_1,\\ldots,y_n$?  Compute the MLE from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DERIVATIONS AND INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a general setting, assuming that the observations are i. id, the likelihood \n",
    "$$L(\\theta | y_1,\\ldots,y_n) = \\prod_{i=1}^n{p(y_i|\\theta)}$$\n",
    "In this Bernoulli setting, we have:\n",
    "$$\n",
    "p(y_i|\\theta) = \\theta^{y_i}(1-\\theta)^{1-y_i}\n",
    "$$\n",
    "Therefore, the expression of the likelihood is:\n",
    "$$\n",
    "L(\\theta | y_1,\\ldots,y_n) = \\theta^{\\sum_{i=1}^ny_i}(1-\\theta)^{n-\\sum_{i=1}^ny_i}\n",
    "$$\n",
    "In order to find the MLE, we will study the log-likelihood. Indeed, since log is strctly increasing, the **position** of the maximum of the likelihood is the same as the **position** of the log-likelihood.\n",
    "\n",
    "$$\n",
    "log(L(\\theta | y_1,\\ldots,y_n)) = \\mathcal{L}(\\theta) = \\sum_{i=1}^ny_ilog(\\theta) + (n-\\sum_{i=1}^ny_i)log(1-\\theta)\n",
    "$$\n",
    "In order to find the parameter $\\theta$, we need to solve the optimization problem:\n",
    "$$\n",
    "\\theta^{MLE} = argmax_{\\theta \\in [0;1]}\\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "Let us now resolve the free-optimization problem (ie unconstrained) and check whether the optimal parameter found verify the constraints. In order to solve the free optimization problem, we will leverage first-order and second-order methods. \n",
    "\n",
    "$$\n",
    "\\nabla \\mathcal{L} = \\frac{\\sum_{i=1}^ny_i}{\\theta} - \\frac{n-\\sum_{i=1}^ny_i}{1-\\theta} = 0 \\iff (1-\\theta)(\\sum_{i=1}^ny_i) - \\theta(n-\\sum_{i=1}^ny_i) = 0 \\iff \\theta = \\frac{\\sum_{i=1}^ny_i}{n} \n",
    "$$\n",
    "Let us check that $\\sum_{i=1}^ny_i \\leq \\sum_{i=1}^ny_i \\leq n : 0 \\leq \\frac{\\sum_{i=1}^ny_i}{n} \\leq 1$ \n",
    "\n",
    "Last, $\\nabla^2 \\mathcal{L}(\\theta) < 0 \\quad \\forall \\theta \\in [0; 1]$: this is a global optimum. \n",
    "\n",
    "Conclusion : $\\theta^{MLE} = \\frac{\\sum_{i=1}^ny_i}{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MLE of the Bernoulli Parameter is  0.38986556359875907\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "data = pd.read_csv('data/dataset_2_train.csv')\n",
    "y = data.contraceptive_use\n",
    "MLE = np.mean(y)\n",
    "print('The MLE of the Bernoulli Parameter is ', MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sA2\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[A2](#qA2)**\n",
    "\n",
    "</div>\n",
    "\n",
    "Using rejection sampling, simulate a sample of 10,000 accepted values $\\theta$ from the posterior distribution.  Plot a histogram of these values, and provide numerical summaries of the distribution of the 10,000 values.  Interpret your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In rejection sampling, we sample from the posterior thanks to a closed-form distribution from which we can sample. The condition is that this closed-form distribution verifies **posterior = $O$(proposal distribution).** \n",
    "\n",
    "What is convenient when studying the posterior of a Bayesian Model is that we specify the prior, with a closed-form distribution, so we know how to sample from it. Since $p(\\theta | y) = L(y|\\theta)*p(\\theta)$, we have $\\frac{p(\\theta | y)}{p(\\theta)} \\leq M =  L(y|\\theta^{MLE})$.\n",
    "\n",
    "Therefore, the rejection sampling is **tractable** with the prior as a proposal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the posterior through Rejection sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_prior(size=1):\n",
    "    while True:\n",
    "        theta = np.random.normal(loc=0.5, scale=0.5, size=size)\n",
    "        if 0 <= theta and theta <= 1:\n",
    "            return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_likelihood(theta, y):\n",
    "    s = np.sum(y)\n",
    "    n = len(y)\n",
    "    return theta**s*(1-theta)**(n-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = bernoulli_likelihood(MLE, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def rejection_proposal(sampler_proposal, likelihood, M, y):\n",
    "    \"\"\"This function implements one step fo rejection sampling, using the prior distribution as a proposal distribution. \n",
    "    Sampler_proposal and likelihood are functions defined.\"\"\"\n",
    "    theta_proposal = sampler_proposal()\n",
    "    quotient = likelihood(theta_proposal, y)\n",
    "    height_acceptance = M*np.random.uniform()\n",
    "    if quotient <= height_acceptance:\n",
    "        return theta_proposal\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = []\n",
    "count = 0\n",
    "while len(posterior_samples) < 10000:\n",
    "    theta_rejection_sampling = rejection_proposal(sampling_prior, bernoulli_likelihood, M, y)\n",
    "    if theta_rejection_sampling is not None:\n",
    "        posterior_samples.append(theta_rejection_sampling)\n",
    "    count +=1\n",
    "acceptance_rate = 10000/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, figsize =(15, 10))\n",
    "ax.hist(posterior_samples, bins=100, color='lightcoral')\n",
    "plt.title('Histogram of the posterior distribution, sampled thanks to 10000 rejection sampling steps, with an acceptance rate of ' + str(np.round(acceptance_rate, 3)))\n",
    "ax.set_xlabel('Theta Posterior')\n",
    "ax.set_ylabel('Histogram of the posterior distribution')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Summaries of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The posterior mean is  0.5035941086233141\n",
      "The posterior 95% percentiles are  [0.03147366 0.96693159]\n",
      "The Mode of the posterior Distribution is  [0.4997]\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "Mean = np.mean(posterior_samples)\n",
    "Percentiles = np.percentile(posterior_samples, [2.5, 97.5])\n",
    "Mode = scipy.stats.mode(np.round(posterior_samples, 4))\n",
    "\n",
    "print('The posterior mean is ', Mean)\n",
    "print('The posterior 95% percentiles are ', Percentiles)\n",
    "print('The Mode of the posterior Distribution is ', Mode[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is easy to summary the posterior distribution with numerical quantities. The only 'open-ended' question is how to numerically the mode of a distribution:\n",
    "- We could visually determine it, in that case we would discover that the posterior is bi-modal\n",
    "- Compute it, and therefore locate the biggest mode in the posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "We have found samples for the posterior distribution that allowed us to create a histogram of it. Furthermore, we have computed 95% **credible** intervals for the parameter $\\theta$. \n",
    "Several things need to be pointed out:\n",
    "- The **acceptance rate** of the rejection sampling procedure: very close to 1. This should point out to the fact that the likelihood is quite constant accross the entire domain [0; 1]. Let us check this by studying **Variations of the likelihood**.\n",
    "    Since the likelihood is strictly concave, the derivative is non-increasing, meaning that it is initially positive then negative. This means that the original function is increasing and then decreasing. Therefore, the maximum variation is obtained when computing $max(L(\\theta^{MLE})-L(0), L(\\theta^{MLE})-L(1))$. We realise that this value is low, which explains the low variation in the likelihood function, which explains why the acceptance rate is that high\n",
    "\n",
    "- The Bimodality of the posterior: We realize that the posterior is bimodal. \n",
    "\n",
    "- Credible Intervals for the posterior: we can se that the credible intervals are very wide, meaning that we do not have that much information in oru posterior (we knew that $\\theta$ was in [0; 1], now we can affirm with 95% certainty that it is in [0.03147366; 0.96693159], and all the values in this interval seem **fairly distributed**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sA3\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[A3](#qA3)**\n",
    "\n",
    "</div>\n",
    "\n",
    "Carry out the weighted bootstrap to simulate 1,000 values of $\\theta$ from the posterior distribution.  In doing so, simulate 10,000 values from the prior distribution to use as the discrete distribution from which the posterior draws will be simulated via the importance weights.  As above, plot a histogram of these values, and provide numerical summaries of the distribution of 1,000 values.  Interpret the results, and compare to the results of rejection sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "posterior_IS = []\n",
    "while len(posterior_IS) < 1000:\n",
    "    thetas_sampled = []\n",
    "    while len(thetas_sampled) < 10000:\n",
    "        theta_sampled = sampling_prior()\n",
    "        if theta_sampled is not None:\n",
    "            thetas_sampled.append(theta_sampled)\n",
    "    thetas_sampled = np.array(thetas_sampled).reshape(10000,)\n",
    "    importance_weight = bernoulli_likelihood(thetas_sampled, y)\n",
    "    q = importance_weight/np.sum(importance_weight)\n",
    "    theta_IS = np.random.choice(a=thetas_sampled, p=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize =(15, 10))\n",
    "ax.hist(posterior_IS, bins=100, color='lightblue')\n",
    "plt.title('Histogram of the posterior distribution, sampled thanks to 1000 Importance sampling steps, created with 10000 prior distribution steps')\n",
    "ax.set_xlabel('Theta Posterior')\n",
    "ax.set_ylabel('Histogram of the posterior distribution')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean = np.mean(posterior_IS)\n",
    "Percentiles = np.percentile(posterior_IS, [2.5, 97.5])\n",
    "Mode = scipy.stats.mode(np.round(posterior_IS, 4))\n",
    "\n",
    "print('The posterior mean is ', Mean)\n",
    "print('The posterior 95% percentiles are ', Percentiles)\n",
    "print('The Mode of the posterior Distribution is ', Mode[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# Part B: Bayesian Logistic Regression and Varying-Intercepts Model\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2intro\"></a>\n",
    "\n",
    "## Overview \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "The second model we will fit to the contraceptives data is a varying-intercept logistic regression model, where the intercept varies by district.\n",
    "\n",
    "Prior distribution:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\beta_{0j} &\\sim N(\\mu_0,\\sigma_0^2)\\; \\text{, with} \\;\\; \\mu_0 \\sim N(0,10000)\\; \\text{and} \\; \\; \\frac{1}{\\sigma^2_0} \\sim \\text{Gamma}(0.1,0.1)\n",
    "\\\\\n",
    "\\beta_1 &\\sim N(0,10000) \n",
    "\\\\ \\\\ \n",
    "\\beta_2 &\\sim N(0,10000)\n",
    "\\\\ \\\\ \n",
    "\\beta_3 &\\sim N(0,10000)\n",
    "\\\\ \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Model for data:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "Y_{ij} & \\sim \\text{Bernoulli}(p_{ij})\n",
    "\\\\ \\\\\n",
    "\\text{logit}\\:p_{ij} &= \\beta_{0j} + \\beta_1 \\times \\text{urban} + \\beta_2 \\times \\text{living.children} + \\beta_3 \\times \\text{age-mean}\n",
    "\\\\ \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where $Y_{ij}$ is 1 if woman $i$ in district $j$ uses contraceptives, and 0 otherwise, and where $i \\in \\{1,...,N\\}$ and $j \\in \\{1,...,J\\}$. $N$ is the number of observations in the data, and $J$ is the number of districts. The above notation assumes $N(\\mu, \\sigma^2)$ is a Normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "**PLEASE NOTE:** The $\\text{Gamma}$ distribution cited above, uses the $\\text{Gamma}(\\alpha, \\beta)$ parametrization, where $\\alpha$ is the shape and $\\beta$ is the rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2questions\"></a>\n",
    "\n",
    "### <div class='exercise'>Part B: Questions</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "<a id=\"qB1\"></a>\n",
    "\n",
    "**[B1](#sB1)** \n",
    "\n",
    "As a preprocessing step, map the district number 61 to the number 54 so that the districts are in order. Also, re-name district 60 to be district 0 so that the districts are numbered 0 through 59. **We use this numbering throughout the homework, and will grade using these district numbers**\n",
    "\n",
    "<a id=\"qB2\"></a>\n",
    "\n",
    "**[B2](#sB2)**\n",
    "\n",
    "First, we'll verify that pymc3 can recover the hidden parameter values. To do so, we'll hard-code known values and simulate data from the model, then using that data we'll check if pymc3 can get back the parameters we hard-coded. If it does, we'll have hope that it can get the hidden parameter values that generated the real data.\n",
    "\n",
    "Use the following hard-coded values:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mu_0 &= 2\n",
    "\\\\ \\\\\n",
    "\\sigma^2_0 &= 1\n",
    "\\\\ \\\\\n",
    "\\beta_{0j} &\\sim N(\\mu_0,\\sigma_0^2) \\; \\text{for all sixty districts}\n",
    "\\\\ \\\\\n",
    "\\beta_1 &= 4\n",
    "\\\\ \\\\ \n",
    "\\beta_2 &= -3\n",
    "\\\\ \\\\\n",
    "\\beta_3 &= -2\n",
    "\\\\ \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "**Verify that these parameters give an overall average of 46 to 48 percent of subjects using contraceptives**\n",
    "\n",
    "<a id=\"qB3\"></a>\n",
    "\n",
    "**[B3](#sB3)**\n",
    "\n",
    "Fit the varying-intercept model specified above to your simulated data. Reasonably good convergence may require adjustments to the number of tuning samples and the target acceptance rate as suggested in pymc3's warnings.\n",
    "\n",
    "<a id=\"qB4\"></a>\n",
    "\n",
    "**[B4](#sB4)** \n",
    "\n",
    "Plot the trace plots of the MCMC sampler for the parameters $\\mu_0, \\frac{1}{\\sigma^2_0}\\text{ (a.k.a., }\\tau_0\\text{)}, \\beta_1, \\beta_2, \\beta_3$. Based on these and the R-hat values, does it look like the samplers converged?\n",
    "\n",
    "<a id=\"qB5\"></a>\n",
    "\n",
    "**[B5](#sB5)**\n",
    "\n",
    "Plot histograms of the posterior distributions for the parameters $\\beta_{0,9}$, $\\beta_{0,19}$, $\\beta_{0,29}$, ..., $\\beta_{0,59}$. Are the actual parameters that you generated contained within these posterior distributions? \n",
    "\n",
    "<a id=\"qB6\"></a>\n",
    "\n",
    "**[B6](#sB6)**\n",
    "\n",
    "We now fit our model to the actual data. Fit the varying-intercept model to the real training data.\n",
    "\n",
    "<a id=\"qB7\"></a>\n",
    "\n",
    "**[B7](#sB7)** \n",
    "\n",
    "Check the convergence by examining the trace plots and R-hats, as you did with the simulated data. What do you observe?\n",
    "\n",
    "<a id=\"qB8\"></a>\n",
    "\n",
    "**[B8](#sB8)**\n",
    "\n",
    "Based on the posterior means, which district has the highest base rate of contraceptive usage (independent of other factors like urban population)? Which district has the lowest?\n",
    "\n",
    "<a id=\"qB9\"></a>\n",
    "\n",
    "**[B9](#sB9)**\n",
    "\n",
    "What are the posterior means of $\\mu_0$ and $\\sigma_0$? Do these values offer any evidence in support of or against the varying-intercept model, compared to a model with a single intercept value for all districts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2solutions\"></a>\n",
    "\n",
    "## Part B: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sB1\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[B1](#qB1)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "As a preprocessing step, map the district number 61 to the number 54 so that the districts are in order. Also, re-name district 60 to be district 0 so that the districts are numbered 0 through 59. **We use this numbering throughout the homework, and will grade using these district numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sB2\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[B2](#qB2)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "First, we'll verify that pymc3 can recover the hidden parameter values. To do so, we'll hard-code known values and simulate data from the model, then using that data we'll check if pymc3 can get back the parameters we hard-coded. If it does, we'll have hope that it can get the hidden parameter values that generated the real data.\n",
    "\n",
    "Use the following hard-coded values:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mu_0 &= 2\n",
    "\\\\ \\\\\n",
    "\\sigma^2_0 &= 1\n",
    "\\\\ \\\\\n",
    "\\beta_{0j} &\\sim N(\\mu_0,\\sigma_0^2) \\; \\text{for all sixty districts}\n",
    "\\\\ \\\\\n",
    "\\beta_1 &= 4\n",
    "\\\\ \\\\ \n",
    "\\beta_2 &= -3\n",
    "\\\\ \\\\\n",
    "\\beta_3 &= -2\n",
    "\\\\ \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "**Verify that these parameters give an overall average of 46 to 48 percent of subjects using contraceptives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sB3\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[B3](#qB3)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Fit the varying-intercept model specified above to your simulated data. Reasonably good convergence may require adjustments to the number of tuning samples and the target acceptance rate as suggested in pymc3's warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sB4\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[B4](#qB4)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Plot the trace plots of the MCMC sampler for the parameters $\\mu_0, \\frac{1}{\\sigma^2_0}\\text{ (a.k.a., }\\tau_0\\text{)}, \\beta_1, \\beta_2, \\beta_3$. Based on these and the R-hat values, does it look like the samplers converged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sB5\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[B5](#qB5)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Plot histograms of the posterior distributions for the parameters $\\beta_{0,9}$, $\\beta_{0,19}$, $\\beta_{0,29}$, ..., $\\beta_{0,59}$. Are the actual parameters that you generated contained within these posterior distributions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sB6\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[B6](#qB6)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "We now fit our model to the actual data. Fit the varying-intercept model to the real training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sB7\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[B7](#qB7)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Check the convergence by examining the trace plots and R-hats, as you did with the simulated data. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sB8\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[B8](#qB8)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Based on the posterior means, which district has the highest base rate of contraceptive usage (independent of other factors like urban population)? Which district has the lowest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sB9\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[B9](#qB9)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "What are the posterior means of $\\mu_0$ and $\\sigma_0$? Do these values offer any evidence in support of or against the varying-intercept model, compared to a model with a single intercept value for all districts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# Part C: Varying-Coefficients Model and Model Selection\n",
    "\n",
    "[Return to contents](#contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3intro\"></a>\n",
    "\n",
    "## Overview \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "The next model we will fit to the contraceptives data is a varying-coefficients logistic regression model, where the coefficients on living.children, age-mean, and urban vary by district.\n",
    "\n",
    "Prior distribution:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\beta_{0j} &\\sim N(\\mu_0,\\sigma_0^2)\\; \\text{, with} \\;\\; \\mu_0 \\sim N(0,10000)\\; \\text{and} \\; \\; \\frac{1}{\\sigma^2_0} \\sim \\text{Gamma}(0.1,0.1)\n",
    "\\\\\n",
    "\\beta_{1j} &\\sim N(0,\\sigma_1^2)\\; \\text{, with} \\;\\; \\frac{1}{\\sigma^2_1} \\sim \\text{Gamma}(0.1,0.1) \n",
    "\\\\\n",
    "\\beta_{2j} &\\sim N(0,\\sigma_2^2)\\; \\text{, with} \\;\\; \\frac{1}{\\sigma^2_2} \\sim \\text{Gamma}(0.1,0.1)\n",
    "\\\\ \n",
    "\\beta_{3j} &\\sim N(0,\\sigma_3^2)\\; \\text{, with} \\;\\; \\frac{1}{\\sigma^2_3} \\sim \\text{Gamma}(0.1,0.1)\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Model for data:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "Y_{ij} &\\sim \\text{Bernoulli}(p_{ij})\n",
    "\\\\ \\\\\n",
    "\\text{logit}\\:p_{ij} &= \\beta_{0j} + \\beta_{1j} \\times \\text{urban} + \\beta_{2j} \\times \\text{living.children} + \\beta_{3j} \\times \\text{age-mean}\n",
    "\\\\ \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "**PLEASE NOTE:** Once again, the $\\text{Gamma}$ distribution uses the $\\text{Gamma}(\\alpha, \\beta)$ parametrization, where $\\alpha$ is the shape and $\\beta$ is the rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3questions\"></a>\n",
    "\n",
    "### <div class='exercise'>Part C: Questions</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "<a id=\"qC1\"></a>\n",
    "\n",
    "**[C1](#sC1)** \n",
    "\n",
    "Fit the varying-coefficients model described above to the real training data.\n",
    "\n",
    "<a id=\"qC2\"></a>\n",
    "\n",
    "**[C2](#sC2)** \n",
    "\n",
    "Check the convergence of your varying-coefficients model by examining the trace plots and R-hats, as you did with the prior varying-intercepts model. What do you observe?\n",
    "\n",
    "<a id=\"qC3\"></a>\n",
    "\n",
    "**[C3](#sC3)** \n",
    "\n",
    "Plot the distributions of posterior means and credible intervals for each predictor's coefficient by district. What do you conclude from these graphs?\n",
    "\n",
    "**HINT:** The ArviZ [`plot_forest()`](https://arviz-devs.github.io/arviz/api/generated/arviz.plot_forest.html) function is very well-suited for this task.\n",
    "\n",
    "<a id=\"qC4\"></a>\n",
    "\n",
    "**[C4](#sC4)** \n",
    "\n",
    "Use all of the information you've gleaned thus far to build Bayesian logistic regression classifiers for both your varying-intercepts model (from B7) and your varying-coefficients model (from C1). Then, use each model to make predictions on your training and test sets.\n",
    "\n",
    "  - Report each model's classification percentages and accuracy scores on both the training and test sets, as well as the trivial accuracy scores you would achieve with a \"naive\" model that predicts only the most frequent outcome observed in your training data.\n",
    "  \n",
    "  \n",
    "  - What do you observe from these results?\n",
    "  \n",
    "  \n",
    "  - Which model appears to be the best (i.e. varying-intercept or varying-coefficient), and what is your rationale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3solutions\"></a>\n",
    "\n",
    "## Part C: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sC1\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[C1](#qC1)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Fit the varying-coefficients model described above to the real training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sC2\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[C2](#qC2)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Check the convergence of your varying-coefficients model by examining the trace plots and R-hats, as you did with the prior varying-intercepts model. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sC3\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[C3](#qC3)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Plot the distributions of posterior means and credible intervals for each predictor's coefficient by district. What do you conclude from these graphs?\n",
    "\n",
    "**HINT:** The ArviZ [`plot_forest()`](https://arviz-devs.github.io/arviz/api/generated/arviz.plot_forest.html) function is very well-suited for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sC4\"></a>\n",
    "<div class='exercise-r'>  \n",
    "\n",
    "**[C4](#qC4)**\n",
    "    \n",
    "</div>\n",
    "\n",
    "Use all of the information you've gleaned thus far to build Bayesian logistic regression classifiers for both your varying-intercepts model (from B7) and your varying-coefficients model (from C1). Then, use each model to make predictions on your training and test sets.\n",
    "\n",
    "  - Report each model's classification percentages (i.e. percentage of class `1` predictions) and accuracy scores on both the training and test sets, as well as the trivial accuracy scores you would achieve with a \"naive\" model that predicts only the most frequent outcome observed in your training data.\n",
    "  \n",
    "  \n",
    "  - What do you observe from these results?\n",
    "  \n",
    "  \n",
    "  - Which model appears to be the best (i.e. varying-intercept or varying-coefficient), and what is your rationale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
