{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: <br>Advanced Topics in Data Science\n",
    "\n",
    "## Homework 6 - Language Modelling and Text Classification\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2021**<br/>\n",
    "**Instructors**: Mark Glickman, Pavlos Protopapas, and Chris Tanner <br/>\n",
    "**Release Date**: March 24, 2021<br/>\n",
    "**Due Date**: <font color=\"red\">April 7 (11:59pm EST), 2021</font><br/>\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/\"\n",
    "    \"content/styles/cs109.css\"\n",
    ").text\n",
    "\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "    \n",
    "# Overview \n",
    "\n",
    "In this homework, your goal is to learn the basic principles of language modelling and text classification. As you learned in lecture, these are distinct, independent tasks, but language modelling often yields one with useful representations of language (e.g., word embeddings). Specifically, these representations may be useful toward _other_ NLP tasks, such as text classification, question-answering, named entity recognition, and a slew of other NLP problems.\n",
    "\n",
    "Toward this goal of understanding both language models (LMs) and text classification, you will build from scratch a simple language model (a _bi-gram_ language model). You will then gain experience working with popular pre-trained, open-sourced language models (e.g. `BERT`). Finally, you will develop text classification models for an actual real-life application of assisting in systematic reviews of medical research abstracts. \n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "- Please restart the kernel and run the entire notebook again before you submit.\n",
    "\n",
    "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. **Please use only the libraries provided in those imports.**\n",
    "\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that clearly labels the output, includes a reference to the calculated value, and rounds it to a reasonable number of digits. **Do not hard code values in your printed output**. For example, this is an appropriate print statement:\n",
    "```python\n",
    "print(f\"The R^2 is {R:.4f}\")\n",
    "```\n",
    "- Your plots should be clearly labeled, including clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is NOT a descriptive title; \"95% confidence interval of coefficients of polynomial degree 5\" on the other hand is descriptive).\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### Learning Objectives\n",
    "- Become familiar with language models and what they actually do (going beyond using them as black boxes)\n",
    "- Gain experience with popular, pre-trained language models like `BERT` and `GPT2`.\n",
    "- Understand metrics for assessing the quality of language models\n",
    "- Become comfortable using classifiers on text data\n",
    "- Reflect on what further options you could pursue with the skills you learned in this homework (e.g., Could you implement an auto-complete or auto-correct app for text messages? What else can you do with the embeddings produced from `BERT` or similar models?)\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### Notes\n",
    "- Creating the `BigramLM` requires careful programming, as it's easy to make a mistake. Make sure you allocate appropriate time to complete it.\n",
    "- You will train your own word embeddings. In this part your take away is to understand what embeddings are and create it on a new dataset. \n",
    "- The text classification task is for you to compare and contrast the various modeling techniques. Your goal is to apply deep learning models for classification on a new dataset.\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook Contents\n",
    "\n",
    "- [**Part 0 (Setup Notebook)**](#part0)\n",
    "\n",
    "- [**PART 1 [ 50 pts ]: Language Modelling**](#part1)\n",
    "  - [Overview](#part1intro)\n",
    "  - [Questions](#part1questions)\n",
    "  - [Solutions](#part1solutions)\n",
    "\n",
    "\n",
    "- [**PART 2 [ 15 pts ]: Word Embeddings**](#part2)\n",
    "  - [Overview](#part2intro)\n",
    "  - [Questions](#part2questions)\n",
    "  - [Solutions](#part2solutions)\n",
    "\n",
    "\n",
    "- [**PART 3 [ 35 pts ]: Text Classification**](#part3)\n",
    "  - [Overview](#part3intro)\n",
    "  - [Questions](#part3questions)\n",
    "  - [Solutions](#part3solutions)\n",
    "\n",
    "NOTE: Exact point values may change a little"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part0\"></a>\n",
    "\n",
    "## Part 0. Setup Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Exercise 0 [0 pt]: Install HuggingFace transformers, download datasets</b>\n",
    "\n",
    "[HuggingFace](https://huggingface.co/transformers/) is revolutionary in providing well-coded, open-source implementations of many state-of-the-art models for NLP. Additionally, they have hundreds of corpora available for research and development, too. Be sure you install the transformers package from HuggingFace. (See cell below to download datasets and installing HuggingFace transformers. )\n",
    "    \n",
    "    \n",
    "- Install transformers 4.4.1\n",
    "- Download hw_utils.py (Helper functions for homework)\n",
    "- Download Harry Potter dataset - Part 1&2\n",
    "- Download Medical Abstract dataset - Part 3\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Installation for Jupyter Hub**</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo /usr/share/anaconda3/bin/conda install -c huggingface transformers==4.4.1 -y\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/Harvard-IACS/2021-CS109B/master/content/misc/hw_utils.py\n",
    "\n",
    "# import hw_utils\n",
    "\n",
    "# !mkdir data\n",
    "# hw_utils.download_file('https://cs109b-course-data.s3.amazonaws.com/hw6/harry_potter.zip',extract=True,base_path='data/harry_potter/',)\n",
    "\n",
    "# hw_utils.download_file(\" https://cs109b-course-data.s3.amazonaws.com/project_1.zip\", base_path=\"datasets\", extract=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Installation for Google Colab**</font>\n",
    "\n",
    "Remember to enbale GPU by going to \"Runtime > Change Runtime Type\" and selecting \"GPU\" for Hardware accelarator and then click \"Save\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers==4.4.1\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/Harvard-IACS/2021-CS109B/master/content/misc/hw_utils.py\n",
    "\n",
    "# import hw_utils\n",
    "\n",
    "# !mkdir data\n",
    "# hw_utils.download_file('https://cs109b-course-data.s3.amazonaws.com/hw6/harry_potter.zip',extract=True,base_path='data/harry_potter/',)\n",
    "\n",
    "# hw_utils.download_file(\" https://cs109b-course-data.s3.amazonaws.com/project_1.zip\", base_path=\"datasets\", extract=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of your working environment (e.g.., Google Colab, JupyterHub, locally on your own machine), run the following cell to import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2' #Trying to reduce tensorflow warnings\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import hw_utils # LOADS HW CODE (helps de-clutter this notebook)\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# useful structures and functions for experiments \n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "\n",
    "# specific machine learning functionality\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.utils.layer_utils import count_params\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christanner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christanner/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk's punkt sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "# download nltk's stop words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell prints installation/version details, which is very helpful for debugging potential package issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 2.4.1\n",
      "keras version 2.4.0\n",
      "Eager Execution Enabled: True\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of replicas: 1\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "All Physical Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
    "# without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(\"tensorflow version\", tf.__version__)\n",
    "print(\"keras version\", tf.keras.__version__)\n",
    "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
    "\n",
    "# Get the number of replicas \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(\"Devices:\", devices)\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/data_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "\n",
    "\n",
    "# PART 1. Language Modelling [50 pts]\n",
    "\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "<a id=\"part1intro\"></a>\n",
    "## Overview\n",
    "\n",
    "Recall from lecture that language models (LMs), by definition, allow us to compute the probability of any given sequence $S$. In doing so, they implicitly allow us to compute the most probable <i>next</i> word, too (i.e., given a sequence of length $s_{i}$, the most likely next word is that which gives us the most likely sequence of length $s_{i+1}$.\n",
    "\n",
    "Let's build a language model from scratch, starting with the most simple one, a `unigram` model. Next, you will build a `bi-gram` model. You'll also use a pre-trained `GPT-2` model. For all of these language models, we will use the text from Harry Potter books. You will evaluate these models both objectively, via perplexity, and subjectively, via your impressions of their outputs.\n",
    "\n",
    "\n",
    "<a id=\"part1questions\"></a>\n",
    "\n",
    "## PART 1: Questions\n",
    "\n",
    "\n",
    "<a id=\"q11\"></a>\n",
    "\n",
    "**[1.1:](#s11)** **Preprocessing + Base Function for Optimization** <font color='red'>(do not edit)</font>\n",
    "\n",
    "<a id=\"q12\"></a>\n",
    "\n",
    "**[1.2:](#s12)** **Unigram Model** <font color='red'>(do not edit)</font>\n",
    "\n",
    "Below, we provide the functionality for a unigram LM. It inherits the `optimize_parameters()` method from the base class, `NGramModel`. Notice that `optimize_parameters()` expects one to pass a class-specific `calculate_likelihood()` method to it. For example, our `UnigramModel` class defines its own `calculate_likelihood()` method, which accepts an `alpha` parameter. To be clear, the optimal `alpha` value is determined by `optimize_parameters()`.\n",
    "\n",
    "The `UnigramModel` constructor only accepts a list of tokens (should always be the training tokens). For clarity, we now briefly describe each of its methods:\n",
    "\n",
    "* `convert_tokens()` simply outputs the same input tokens it received, but after optionally UNKifying (\\*U\\*) the ones that were not present within the training tokens. Thus, the training tokens are left unedited, but any **dev** or **test** tokens will need to be UNKified.\n",
    "\n",
    "* `get_word_counts()` returns a dictionary-style count of all tokens (_key_ = word type; _value_ = # of occurrences)\n",
    "\n",
    "* `calculate_likelihood()` returns the unigram likelihood (negative log likelihood) of the passed-in tokens, based on the training data.\n",
    "    \n",
    "\n",
    "<a id=\"q13\"></a>\n",
    "**[1.3:](#s13)** **Bigram Model** <font color='red'>(you create)</font>  \n",
    "\n",
    "<a id=\"q131\"></a>\n",
    "\n",
    "**[1.3.1:](#s131)** **Implementing a Bigram LM**\n",
    "    \n",
    "Your task is to write a Bigram model below. You have full freedom to design it however you wish, including adding any methods you wish, as long it can be executed by the cell that succeeds it. That is:\n",
    "\n",
    "* the `BigramModel` constructor must accept a list of training tokens, just like how we instantiate the `UnigramModel`.\n",
    "\n",
    "* you must provide functionality to return `dev_bigram_counts` (a `Counter` or `dict` of the bigram tokens).\n",
    "\n",
    "* you must write `calculate_likelihood()`, which accepts and works with `dev_bigram_counts`.\n",
    "\n",
    "* using our provided code, you'll find the optimal $\\beta$ (while using the optimal $\\alpha$ found from the UnigramModel)\n",
    "\n",
    "* edit the \"BIGRAM TESTING\" CELL so that you can actually run your BigramModel\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "* You can design the structure of your bigram tokens to be any format you wish. I find it useful to stitch them together with an `_`, since an underscore is never present within our corpus. Using tuples would also be great.\n",
    "\n",
    "* To be clear, let's say our **dev set** is `demo_dev.txt`, which has one sentence: __I love NLP.__ The above `parse_file()` will convert this to `dev_data`, a list of six tokens: **[\\< s\\>, i, love, nlp, ., \\< s \\>]**. Regardless of how you format the bigram tokens (e.g., tuples or concatenated with `_` or other special characters), your code should produce `dev_bigram_counts` as having five distinct keys/bigrams (illustrated here with the `_` format):\n",
    "\n",
    "`Counter({'\\< s\\>\\_i': 1, 'i_love': 1, 'love_\\*U\\*': 1, '\\*U\\*_.': 1, '._\\< s \\>': 1})`\n",
    "\n",
    "* If you set the **train set** to be `demo_train.txt` and the **dev set** to be `demo_dev.txt`, then running the above UnigramModel will yield you with an optimal $\\alpha$ of 4.97. The last line of the **BIGRAM TESTING** cell will invoke `bigram_lm.optimize_parameters()`. If you implemented the Bigram model correctly, this should find the optimal $\\beta$ to be 1.97 (when using $\\alpha$ = 4.97). Also, when using an $\\alpha$ of 1 and a $\\beta$ of 1, your Bigram should produce a likelihood of 6.6.\n",
    "\n",
    "* Ultimately, you need to run your code on the provided Harry Potter datasets, but the `demo_*` files are provided merely as a sanity check.\n",
    "\n",
    "\n",
    "<a id=\"q132\"></a>\n",
    "\n",
    "**[1.3.2:](#s132)** **Evaluate text**\n",
    "    \n",
    "This is a continuation of your ongoing work. That is, we're still concerning your Bigram LM, which was fit on `training_data` and the optimal $\\alpha$ and $\\beta$ values were based on the `dev_data`.\n",
    "\n",
    "Now, your task is to implement the perplexity metric, so that we have a standardized way of evaluating our Bigram LM. Specifically:\n",
    "1. Add a method `calculate_perplexity()` to your `BigramModel` class above, then\n",
    "2. Edit the code cell below to get the perplexity scores for two different texts, `test_data1` and `test_data2`.\n",
    "3. Print to the screen these two perplexity scores.\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "* As a reminder, perplexity is defined by $2^{-l}$, where $l = \\frac{1}{M} \\sum_{i=1}^{m}\\text{log}(p(w_{i}))$.\n",
    "\n",
    "* Notice, the $\\text{log}(p(w_{i}))$ part of perplexity equation is just **log-likelihood.** In the previous exercise, you computed the **negative log-likelihood.**\n",
    "\n",
    "* $M$ represents the size of our data that is being evaluated.\n",
    "\n",
    "\n",
    "<a id=\"q133\"></a>\n",
    "    \n",
    "**[1.3.3:](#s133)** **Interpret results**\n",
    "    \n",
    "Reflect on the two perplexity scores you received in the previous cell. Specifically, answer and discuss the following (1 or 2 sentences per question is sufficient, no need for fluff please):\n",
    "\n",
    "1. Do the perplexity scores seem reasonable to you? (i.e., would you expect higher or lower values)?\n",
    "2. Relative to each other, would you expect the perplexity scores to be switched -- in terms of which test set yielded the higher or lower perplexity? Why or why not?\n",
    "3. Imagine we significantly trimmed the `test_data2` data to being only half in size. What would you expect to happen to its perplexity score?\n",
    "4. In our work above, we trained on `training_data`, which is the first Harry Potter book. We found the optimal $\\alpha$ and $\\beta$ values based on `dev_data`, which is the second Harry Potter Book. One of the perplexity scores corresponds to `test_data2`, which is the first Lord of the Rings book. However, instead, if we let `dev_data` be a different Lord of the Rings book (e.g., the __second__ Lord of the Rings book), what would you expect to happen to our perplexity score for `test_data2`?\n",
    "\n",
    "\n",
    "<a id=\"q14\"></a>\n",
    "\n",
    "**[1.4:](#s14)** **N-Gram Text Generation**\n",
    "\n",
    " \n",
    "We now have some objective, quantitative indication of our `Unigram`'s and `Bigrams`' abilities to _model_ language. Let's additionally, subjectively inspect how well it can actually generate text.\n",
    "\n",
    "We should generate text according to the following:\n",
    "\n",
    "* use the optimal $\\alpha$ and $\\beta$ values that we learned from the original `dev_data` set (Harry Potter Book #2)\n",
    "* **probabilistically** (not deterministically) generate each token. That is, we don't simply generate the maximum likely token at each time step. Instead, we randomly sample which token to generate, based on all tokens' likelihood.\n",
    "* exclude the possibility of generating an UNK (\\*U\\*) token\n",
    "* force our model to start w/ a \\< s\\> token\n",
    "* stop once our model has generated a total of $N+1$ \\< s\\> tokens (i.e., $N$ sentences in total)\n",
    "\n",
    "Below, we provide code that __probabilistically__ generates five sentences, using our `Unigram` LM.\n",
    "    \n",
    "\n",
    "<a id=\"q141\"></a>\n",
    " \n",
    "    \n",
    "**[1.4.1:](#s141)** **Interpret results**\n",
    "\n",
    "As you can see, the UnigramLM generates pretty non-sensical text. Write 2-3 sentences about the lengths of the sentences that are generated from a UnigramLM. Specifically, are the sentence lengths, on average, expected to be shorter than, equal to, or longer than sentences seen in the training data. How would this change as a function of the size of the training corpus?\n",
    "\n",
    "\n",
    "<a id=\"q142\"></a>\n",
    " \n",
    "    \n",
    "**[1.4.2:](#s142)** **Bigram Text Generation**\n",
    "    \n",
    "Write code below to generate $N$ sentences from your `BigramLM`, a la the UnigramLM text generation above. Your generation should adhere to the same requirements as listed above. That is:\n",
    "    \n",
    "* use the optimal $\\alpha$ and $\\beta$ values that we learned from the original `dev_data` set (Harry Potter Book #2)\n",
    "* **probabilistically** (not deterministically) generate each token. That is, we don't simply generate the maximum likely token at each time step. Instead, we randomly sample which token to generate, based on all tokens' likelihood.\n",
    "* exclude the possibility of generating an UNK (\\*U\\*) token\n",
    "* force your model to start w/ a \\< s\\> token\n",
    "* stop once your model has generated a total of $N+1$ \\< s\\> tokens (i.e., $N$ sentences in total)\n",
    "\n",
    "Use your code to generate **five** sentences.\n",
    "\n",
    "\n",
    "<a id=\"q143\"></a>\n",
    "\n",
    "**[1.4.3:](#s143)** **Interpret results**\n",
    "\n",
    "Reflect on your BigramLM's output. Please write 3-6 sentences, in total (not per bulleted point), about:\n",
    "1. its semantic and syntactic quality compared to the UnigramLM's output\n",
    "2. its expected sentence length compared to the training corpus\n",
    "3. explain one or two weaknesses with generating text with n-gram models\n",
    "\n",
    "\n",
    "<a id=\"q15\"></a>\n",
    "\n",
    "**[1.5:](#s15)** **GPT-2**\n",
    "\n",
    "In lecture, we learned about advanced LMs such as Transformers. Specifically, `GPT-2` is an autogressive LM that uses a Transformer Encoder and Transformer Decoder. It represents words as distributed, contextualized word embeddings. Using attention and self-attention, it's able to do a great job at modelling language. To see its power, let's use GPT-2 to model our Harry Potter corpus!\n",
    "\n",
    "As a reminder, `GPT-2` has been pre-trained on a _vast_ amount of text data (40 gbs). So, we will not train it on Harry Potter from scratch; we will simply __fine-tune__ it on Harry Potter Book #1 (`Book_1_The_Philosophers_Stone.txt`).\n",
    "\n",
    "\n",
    "<a id=\"q151\"></a>\n",
    "\n",
    "\n",
    "**[1.5.1:](#s151)** **Load and create tokenizer for GPT-2**\n",
    "    \n",
    "Load `GPT-2` from `HuggingFace`'s libraries. Then, fine-tune it on our Harry Potter Book #1 (`Book_1_The_Philosophers_Stone.txt`).\n",
    "\n",
    "* Use `GPT2Tokenizer` to tokenize the input text\n",
    "* NOTE: In [GPT2Tokenizer](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer), use `distilgpt2` as the `vocab_file` argument.\n",
    "* Split the input text into into blocks of length **100**\n",
    "* Generate inputs and labels by by shifting the input by one position:\n",
    "    * For example if your tokens are [1,2,3,4,5] then your inputs will be: [1,2,3,4] and labels will be: [2,3,4,5]\n",
    " \n",
    "\n",
    "\n",
    "<a id=\"q152\"></a>\n",
    " \n",
    "    \n",
    "**[1.5.2:](#s152)** **Prepare data for GPT-2**\n",
    "   \n",
    "* Create TF Datasets with your inputs and labels\n",
    "* Use a batch size of 12\n",
    "* When creating `tf.data` pipelines for training dataset. Follow this order when building the pipeline:\n",
    "  * Shuffle\n",
    "  * Batch\n",
    "  * Prefetch\n",
    "  \n",
    "<a id=\"q153\"></a>\n",
    "\n",
    "**[1.5.3:](#s153)** **Finetune GPT-2**\n",
    "\n",
    "* Build a model using `TFGPT2LMHeadModel` from the `transformers` package from Hugging Face\n",
    "* Load the pre-trained weights using `distilgpt2`\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"q154\"></a>\n",
    "    \n",
    "**[1.5.4:](#s154)** **GPT-2 - model training**\n",
    "    \n",
    "* Use the `Adam` optimizer with a `learning_rate = 3e-5`, epsilon = `1e-08` and clipnorm = `1.0`\n",
    "* Use the loss `SparseCategoricalCrossentropy(from_logits=True)`. The pretrained `distilgpt2` has **6** Transformer layers. When fine-tuning our language model, we do not want to compute losses for every layers. So, when defining the loss functions, remember to set `None` for all of the transformer output layers. We will need to pass in an _array_ of loss functions to the `model.compile(...)`, rather than just one.\n",
    "* Use the metrics `SparseCategoricalAccuracy('accuracy')`\n",
    "* Train the model for at least **30 epochs**. The more you train, the better it will be at generating high-quality text (as you will see in the next question).\n",
    "\n",
    "\n",
    "<a id=\"q155\"></a>\n",
    "\n",
    "**[1.5.5:](#s155)** **Generate text with GPT-2**\n",
    "\n",
    "Now, using your GPT-2 model, generate 5 sentences of Harry Potter text!\n",
    "    \n",
    "* Use a prompt (2-3 words) and tokenize the words\n",
    "* Use the `model.generate(...)` function to generate output text\n",
    "* Here's an example of text generated from the fine-tuned GPT-2 model:\n",
    "    * `hufflepuff hates the slytherins.” “but what about you slytherin they hate me.” “oh they hate me!” said harry. “well i’ve got to go — i’ve got to go hogwarts!” “oh you are not going to be allowed goin’ around hogwarts is that funny professor — you know you can’t afford to go off to the bloody baron`\n",
    "    \n",
    "<a id=\"q156\"></a> \n",
    "    \n",
    "**[1.5.6:](#s156)** **Interpret results**\n",
    "    \n",
    "Reflect on your GPT-2's output. Please write 3-5 sentences, in total (not per bulleted point), about:\n",
    "1. its semantic and syntactic quality compared to the NGram LM's output\n",
    "2. weaknesses and strengths you see with the generated text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"part1solutions\"></a>\n",
    "\n",
    "## PART 1: Solutions\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "<a id=\"s11\"></a>\n",
    "\n",
    "\n",
    "## **[1.1:](#q11)** **Preprocessing + Base Function for Optimization** <font color='red'>(do not edit)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'i', 'love', 'cs109b', '.', '<s>', 'i', 'love', 'cs109b', '.', '<s>']\n"
     ]
    }
   ],
   "source": [
    "# parses a text file into tokens, while ignoring lines that have a\n",
    "# specific \"Page #\" footer, which is present in the Harry Potter book data\n",
    "def parse_file(file):\n",
    "\n",
    "    # extracts all of the pertinent text, while ignoring the Page # lines\n",
    "    file_contents = open(file, 'r')\n",
    "    text_filtered = re.sub(r'Page \\| \\d+ .*', '', file_contents.read()).replace('\\n', ' ')\n",
    "\n",
    "    # converts the data into a list of tokens, with a\n",
    "    # <s> denoting sentence boundaries\n",
    "    sentences = sent_tokenize(text_filtered)\n",
    "    \n",
    "    # constructs all tokens\n",
    "    tokens = ['<s>']\n",
    "    for sent in sentences:\n",
    "        sent = word_tokenize(sent)\n",
    "        tokens.extend([token.lower() for token in sent])\n",
    "        tokens.append('<s>')\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "#####################################\n",
    "#   represents a generic n-gram model.\n",
    "#\n",
    "#   UnigramModel and BigramModel classes\n",
    "#   will inherit from this class\n",
    "#####################################\n",
    "class NGramModel:\n",
    "    def __init__(self): pass\n",
    "        \n",
    "    def optimize_parameters(self, calculate_likelihood, a, b, c, resphi, counts, alpha=-1):        \n",
    "\n",
    "        if ((c-a) <= .02):        \n",
    "            # checks if we need to optimize for alpha (i.e., the unigram model's case)\n",
    "            if alpha == -1:\n",
    "                f_a = calculate_likelihood(a, counts)\n",
    "                f_b = calculate_likelihood(b, counts)\n",
    "                f_c = calculate_likelihood(c, counts)\n",
    "                \n",
    "            # we already have an alpha and will optimize other params (i.e., bigram's case)\n",
    "            else: \n",
    "                f_a = calculate_likelihood(alpha, a, counts)\n",
    "                f_b = calculate_likelihood(alpha, b, counts)\n",
    "                f_c = calculate_likelihood(alpha, c, counts)\n",
    "\n",
    "            if (f_a <= f_b):\n",
    "                if f_a <= f_c: return a, f_a\n",
    "                return c, f_c\n",
    "            elif f_b <= f_c: return b, f_b\n",
    "            return c, f_c\n",
    "        \n",
    "        x = 0\n",
    "        if c-b > b-a:\n",
    "            x = b + math.floor(resphi*(c-b))\n",
    "        else:\n",
    "            x = b - math.floor(resphi*(b-a))\n",
    "        \n",
    "        # checks if we need to optimize for alpha (i.e., the unigram model's case)\n",
    "        if alpha == -1:\n",
    "            f_b = calculate_likelihood(b, counts)\n",
    "            f_x = calculate_likelihood(x, counts)\n",
    "            \n",
    "        # we already have an alpha and will optimize other params (i.e., bigram's case)\n",
    "        else:\n",
    "            f_b = calculate_likelihood(alpha, b, counts)\n",
    "            f_x = calculate_likelihood(alpha, x, counts)\n",
    "        \n",
    "        if f_x < f_b:\n",
    "            if c-b > b-a:\n",
    "                return self.optimize_parameters(calculate_likelihood, b, x, c, resphi, counts, alpha)\n",
    "            return self.optimize_parameters(calculate_likelihood, a, x, b, resphi, counts, alpha)\n",
    "\n",
    "        else:\n",
    "            if c-b > b-a:\n",
    "                return self.optimize_parameters(calculate_likelihood, a, b, x, resphi, counts, alpha)\n",
    "            return self.optimize_parameters(calculate_likelihood, x, b, c, resphi, counts, alpha)\n",
    "\n",
    "##=========================#\n",
    "##   necessary variables   #\n",
    "##=========================#\n",
    "\n",
    "# starting boundaries for Golden-section search algorithm\n",
    "resphi = 2 - (1 + math.sqrt(5)) / 2\n",
    "a = 0.01\n",
    "c = 1000\n",
    "b = resphi*c\n",
    "\n",
    "# constructs data\n",
    "training_file = \"data/harry_potter/demo_train.txt\" # Book_1_The_Philosophers_Stone.txt\" #\n",
    "dev_file = \"data/harry_potter/demo_dev.txt\" #Book_2_The_Chamber_of_Secrets.txt\" #\n",
    "test_file1 = \"data/harry_potter/Book_3_The_Prisoner_of_Azkaban.txt\"\n",
    "test_file2 = \"data/harry_potter/the_fellowship_of_the_ring.txt\"\n",
    "\n",
    "training_data = parse_file(training_file)\n",
    "dev_data = parse_file(dev_file)\n",
    "test_data1 = parse_file(test_file1)\n",
    "test_data2 = parse_file(test_file2)\n",
    "\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s12\"></a>\n",
    "\n",
    "## **[1.2:](#q12)** **Unigram Model** <font color='red'>(do not edit)</font>\n",
    "\n",
    "<div class='exercise'>\n",
    "    \n",
    "Below, we provide the functionality for a unigram LM. It inherits the `optimize_parameters()` method from the base class, `NGramModel`. Notice that `optimize_parameters()` expects one to pass a class-specific `calculate_likelihood()` method to it. For example, our `UnigramModel` class defines its own `calculate_likelihood()` method, which accepts an `alpha` parameter. To be clear, the optimal `alpha` value is determined by `optimize_parameters()`.\n",
    "\n",
    "The `UnigramModel` constructor only accepts a list of tokens (should always be the training tokens). For clarity, we now briefly describe each of its methods:\n",
    "\n",
    "* `convert_tokens()` simply outputs the same input tokens it received, but after optionally UNKifying (\\*U\\*) the ones that were not present within the training tokens. Thus, the training tokens are left unedited, but any **dev** or **test** tokens will need to be UNKified.\n",
    "\n",
    "* `get_word_counts()` returns a dictionary-style count of all tokens (_key_ = word type; _value_ = # of occurrences)\n",
    "\n",
    "* `calculate_likelihood()` returns the unigram likelihood (negative log likelihood) of the passed-in tokens, based on the training data.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel(NGramModel):\n",
    "    def __init__(self, training_tokens):\n",
    "        \n",
    "        self.training_unigrams = self.convert_tokens(training_tokens, False)\n",
    "        self.training_word_counts = self.get_word_counts(self.training_unigrams)\n",
    "\n",
    "    # dev and test tokens should UNK the out-of-vocabulary (OOV) words\n",
    "    def convert_tokens(self, tokens, convert_to_unks):\n",
    "        if convert_to_unks:\n",
    "            tokens = [\"*U*\" if token not in self.training_unigrams else token for token in tokens]      \n",
    "        return tokens\n",
    "    \n",
    "    def get_word_counts(self, tokens): return Counter(tokens)\n",
    "    \n",
    "    def calculate_likelihood(self, alpha, unigram_counts):\n",
    "\n",
    "        likelihood = 0.0\n",
    "\n",
    "        # we add 1 for the *U* type\n",
    "        num_training_types = len(self.training_word_counts) + 1\n",
    "        for token in unigram_counts:\n",
    "            numerator = self.training_word_counts[token] + alpha\n",
    "            denom = len(self.training_unigrams) + alpha*num_training_types\n",
    "            likelihood += -1.0*unigram_counts[token]*math.log(numerator / denom)            \n",
    "\n",
    "        return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's instantiate the model, fit it to our **training** data, and calculate the optimal $\\alpha$ that maximizes the likelihood of the **dev** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructs a unigram LM\n",
    "unigram_lm = UnigramModel(training_data)\n",
    "\n",
    "# parses the dev data. for large corpora, this isn't fast\n",
    "dev_tokens = unigram_lm.convert_tokens(dev_data, True)\n",
    "dev_counts = unigram_lm.get_word_counts(dev_tokens)\n",
    "\n",
    "# determines the MLE for alpha \n",
    "optimal_alpha, optimal_likelihood = \\\n",
    "    unigram_lm.optimize_parameters(unigram_lm.calculate_likelihood, a, b, c, resphi, dev_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal_alpha: 4.97 (dev data's unigram likelihood: 10.7)\n"
     ]
    }
   ],
   "source": [
    "print(f\"optimal_alpha: {optimal_alpha:.2f} (dev data's unigram likelihood: {optimal_likelihood:0.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s13\"></a>\n",
    "\n",
    "## **[1.3:](#q13)** **Bigram Model** <font color='red'>(you create)</font> [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s131\"></a>\n",
    "<div class='exercise'>\n",
    "\n",
    "**[1.3.1:](#q131)** **Implementing a Bigram LM**\n",
    "    \n",
    "Your task is to write a Bigram model below. You have full freedom to design it however you wish, including adding any methods you wish, as long it can be executed by the cell that succeeds it. That is:\n",
    "\n",
    "* the `BigramModel` constructor must accept a list of training tokens, just like how we instantiate the `UnigramModel`.\n",
    "\n",
    "* you must provide functionality to return `dev_bigram_counts` (a `Counter` or `dict` of the bigram tokens).\n",
    "\n",
    "* you must write `calculate_likelihood()`, which accepts and works with `dev_bigram_counts`.\n",
    "\n",
    "* using our provided code, you'll find the optimal $\\beta$ (while using the optimal $\\alpha$ found from the UnigramModel)\n",
    "\n",
    "* edit the \"BIGRAM TESTING\" CELL so that you can actually run your BigramModel\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "* You can design the structure of your bigram tokens to be any format you wish. I find it useful to stitch them together with an `_`, since an underscore is never present within our corpus. Using tuples would also be great.\n",
    "\n",
    "* To be clear, let's say our **dev set** is `demo_dev.txt`, which has one sentence: __I love NLP.__ The above `parse_file()` will convert this to `dev_data`, a list of six tokens: **[\\< s\\>, i, love, nlp, ., \\< s \\>]**. Regardless of how you format the bigram tokens (e.g., tuples or concatenated with `_` or other special characters), your code should produce `dev_bigram_counts` as having five distinct keys/bigrams (illustrated here with the `_` format):\n",
    "> Counter({'\\< s\\>\\_i': 1, 'i_love': 1, 'love_\\*U\\*': 1, '\\*U\\*_.': 1, '._\\< s \\>': 1})\n",
    "\n",
    "* If you set the **train set** to be `demo_train.txt` and the **dev set** to be `demo_dev.txt`, then running the above UnigramModel will yield you with an optimal $\\alpha$ of 4.97. The last line of the **BIGRAM TESTING** cell will invoke `bigram_lm.optimize_parameters()`. If you implemented the Bigram model correctly, this should find the optimal $\\beta$ to be 1.97 (when using $\\alpha$ = 4.97). Also, when using an $\\alpha$ of 1 and a $\\beta$ of 1, your Bigram should produce a likelihood of 6.6.\n",
    "\n",
    "* Ultimately, you need to run your code on the provided Harry Potter datasets, but the `demo_*` files are provided merely as a sanity check.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: FEEL FREE TO ADD ANY METHODS THAT YOU WISH\n",
    "class BigramModel(NGramModel):\n",
    "    def __init__(self, training_tokens):\n",
    "        #### Your code here ####\n",
    "\n",
    "        #### End code here ####\n",
    "                \n",
    "    # returns the bigram head counts (i.e., token#1)\n",
    "    # and the full bigram counts (i.e., token#1_token#2)\n",
    "    def get_word_counts(self, bigrams):\n",
    "        #### Your code here ####\n",
    "\n",
    "        #### End code here ####\n",
    "            \n",
    "    def calculate_perplexity(self, alpha, beta, num_tokens, bigram_counts):\n",
    "        #### Your code here ####\n",
    "\n",
    "        #### End code here ####\n",
    "        \n",
    "    def calculate_likelihood(self, alpha, beta, bigram_counts):\n",
    "                \n",
    "        total_likelihood = 0.0\n",
    "\n",
    "        #### Your code here ####\n",
    "\n",
    "        #### End code here ####\n",
    "            \n",
    "        return total_likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## \"BIGRAM TESTING\" CELL ##\n",
    "###########################\n",
    "\n",
    "# constructs a bigram LM\n",
    "bigram_lm = BigramModel(training_data)\n",
    "\n",
    "#### Your code here ####\n",
    "\n",
    "#### End code here ####\n",
    "\n",
    "# determines the MLE for beta, using the pre-computed optimal unigram alpha\n",
    "optimal_beta, optimal_likelihood = \\\n",
    "    bigram_lm.optimize_parameters(bigram_lm.calculate_likelihood, a, b, c, resphi, dev_bigram_counts, optimal_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal_alpha: 4.97; optimal_beta: 1.97 (dev data's bigram likelihood: 6.1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"optimal_alpha: {optimal_alpha:.2f}; optimal_beta: {optimal_beta:.2f} (dev data's bigram likelihood: {optimal_likelihood:0.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s132\"></a>\n",
    "<div class='exercise'>\n",
    "\n",
    "**[1.3.2:](#q132)** **Evaluate text**\n",
    "    \n",
    "This is a continuation of your ongoing work. That is, we're still concerning your Bigram LM, which was fit on `training_data` and the optimal $\\alpha$ and $\\beta$ values were based on the `dev_data`.\n",
    "\n",
    "Now, your task is to implement the perplexity metric, so that we have a standardized way of evaluating our Bigram LM. Specifically:\n",
    "1. Add a method `calculate_perplexity()` to your `BigramModel` class above, then\n",
    "2. Edit the code cell below to get the perplexity scores for two different texts, `test_data1` and `test_data2`.\n",
    "3. Print to the screen these two perplexity scores.\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "* As a reminder, perplexity is defined by $2^{-l}$, where $l = \\frac{1}{M} \\sum_{i=1}^{m}\\text{log}(p(w_{i}))$.\n",
    "\n",
    "* Notice, the $\\text{log}(p(w_{i}))$ part of perplexity equation is just **log-likelihood.** In the previous exercise, you computed the **negative log-likelihood.**\n",
    "\n",
    "* $M$ represents the size of our data that is being evaluated.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data1's perplexity: 4.15\n",
      "test_data2's perplexity: 4.16\n"
     ]
    }
   ],
   "source": [
    "#### Your code here ####\n",
    "\n",
    "#### End code here ####\n",
    "\n",
    "print(f\"test_data1's perplexity: {test1_perplexity:0.2f}\")\n",
    "print(f\"test_data2's perplexity: {test2_perplexity:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s133\"></a>\n",
    "<div class='exercise'>\n",
    "    \n",
    "**[1.3.3:](#q133)** **Interpret results**\n",
    "    \n",
    "Reflect on the two perplexity scores you received in the previous cell. Specifically, answer and discuss the following (1 or 2 sentences per question is sufficient, no need for fluff please):\n",
    "\n",
    "1. Do the perplexity scores seem reasonable to you? (i.e., would you expect higher or lower values)?\n",
    "2. Relative to each other, would you expect the perplexity scores to be switched -- in terms of which test set yielded the higher or lower perplexity? Why or why not?\n",
    "3. Imagine we significantly trimmed the `test_data2` data to being only half in size. What would you expect to happen to its perplexity score?\n",
    "4. In our work above, we trained on `training_data`, which is the first Harry Potter book. We found the optimal $\\alpha$ and $\\beta$ values based on `dev_data`, which is the second Harry Potter Book. One of the perplexity scores corresponds to `test_data2`, which is the first Lord of the Rings book. However, instead, if we let `dev_data` be a different Lord of the Rings book (e.g., the __second__ Lord of the Rings book), what would you expect to happen to our perplexity score for `test_data2`?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s14\"></a>\n",
    "\n",
    "\n",
    "## **[1.4:](#q14)** **N-Gram Text Generation**\n",
    "\n",
    "<div class='exercise'>\n",
    "We now have some objective, quantitative indication of our `Unigram`'s and `Bigrams`' abilities to _model_ language. Let's additionally, subjectively inspect how well it can actually generate text.\n",
    "\n",
    "We should generate text according to the following:\n",
    "\n",
    "* use the optimal $\\alpha$ and $\\beta$ values that we learned from the original `dev_data` set (Harry Potter Book #2)\n",
    "* **probabilistically** (not deterministically) generate each token. That is, we don't simply generate the maximum likely token at each time step. Instead, we randomly sample which token to generate, based on all tokens' likelihood.\n",
    "* exclude the possibility of generating an UNK (\\*U\\*) token\n",
    "* force our model to start w/ a \\< s\\> token\n",
    "* stop once our model has generated a total of $N+1$ \\< s\\> tokens (i.e., $N$ sentences in total)\n",
    "\n",
    "Below, we provide code that __probabilistically__ generates five sentences, using our `Unigram` LM.\n",
    "    \n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistically generates text from our UnigramLM\n",
    "def generate_unigram_text(unigram_lm, num_sentences):\n",
    "    \n",
    "    # notice, we don't include an UNK (*U*) token, so no +1 adjustments\n",
    "    num_training_unigram_types = len(unigram_lm.training_word_counts)\n",
    "\n",
    "    # precomputes the likelihood of generating each token\n",
    "    token_probs = {}\n",
    "    total = 0\n",
    "    for word_type in unigram_lm.training_word_counts:\n",
    "        numerator = unigram_lm.training_word_counts[word_type] + optimal_alpha\n",
    "        denom = len(unigram_lm.training_unigrams) + optimal_alpha*(num_training_unigram_types)\n",
    "        prob = numerator / denom\n",
    "        token_probs[word_type] = prob\n",
    "        total += prob\n",
    "\n",
    "    # notice that they sum to 1. the same should hold true for your bigram LM\n",
    "    print(\"marginal probability:\", total)\n",
    "\n",
    "    # convenient format for probabilistic sampling\n",
    "    tokens, token_probs = zip(*[(k, token_probs[k]) for k in token_probs.keys()])\n",
    "    tokens = list(tokens)\n",
    "    token_probs = list(token_probs)\n",
    "\n",
    "    # let's generate sentences!\n",
    "    output = \"<s>\"\n",
    "    s_count = 0\n",
    "    while s_count < num_sentences:\n",
    "        rand_token = random.choices(tokens, token_probs)[0]\n",
    "        output += \" \" + rand_token\n",
    "        if rand_token == \"<s>\":\n",
    "            s_count += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_unigram_text(unigram_lm, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s141\"></a>\n",
    "\n",
    "<div class='exercise'> \n",
    "    \n",
    "**[1.4.1:](#q141)** **Interpret results**\n",
    "\n",
    "As you can see, the UnigramLM generates pretty non-sensical text. Write 2-3 sentences about the lengths of the sentences that are generated from a UnigramLM. Specifically, are the sentence lengths, on average, expected to be shorter than, equal to, or longer than sentences seen in the training data. How would this change as a function of the size of the training corpus?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s142\"></a>\n",
    "\n",
    "<div class='exercise'> \n",
    "    \n",
    "**[1.4.2:](#q142)** **Bigram Text Generation**\n",
    "    \n",
    "Write code below to generate $N$ sentences from your `BigramLM`, a la the UnigramLM text generation above. Your generation should adhere to the same requirements as listed above. That is:\n",
    "    \n",
    "* use the optimal $\\alpha$ and $\\beta$ values that we learned from the original `dev_data` set (Harry Potter Book #2)\n",
    "* **probabilistically** (not deterministically) generate each token. That is, we don't simply generate the maximum likely token at each time step. Instead, we randomly sample which token to generate, based on all tokens' likelihood.\n",
    "* exclude the possibility of generating an UNK (\\*U\\*) token\n",
    "* force your model to start w/ a \\< s\\> token\n",
    "* stop once your model has generated a total of $N+1$ \\< s\\> tokens (i.e., $N$ sentences in total)\n",
    "\n",
    "Use your code to generate **five** sentences.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigram_text(bigram_lm, num_sentences):\n",
    "#### Your code here ####\n",
    "\n",
    "#### End code here ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(generate_bigram_text(bigram_lm, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s143\"></a>\n",
    "\n",
    "<div class='exercise'> \n",
    "    \n",
    "**[1.4.3:](#q143)** **Interpret results**\n",
    "\n",
    "\n",
    "Reflect on your BigramLM's output. Please write 3-6 sentences, in total (not per bulleted point), about:\n",
    "1. its semantic and syntactic quality compared to the UnigramLM's output\n",
    "2. its expected sentence length compared to the training corpus\n",
    "3. explain one or two weaknesses with generating text with n-gram models\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s15\"></a>\n",
    "\n",
    "## **[1.5:](#q15)** **GPT-2**\n",
    "\n",
    "<br>\n",
    "    \n",
    "In lecture, we learned about advanced LMs such as Transformers. Specifically, `GPT-2` is an autogressive LM that uses a Transformer Encoder and Transformer Decoder. It represents words as distributed, contextualized word embeddings. Using attention and self-attention, it's able to do a great job at modelling language. To see its power, let's use GPT-2 to model our Harry Potter corpus!\n",
    "\n",
    "As a reminder, `GPT-2` has been pre-trained on a _vast_ amount of text data (40 gbs). So, we will not train it on Harry Potter from scratch; we will simply __fine-tune__ it on Harry Potter Book #1 (`Book_1_The_Philosophers_Stone.txt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s151\"></a>\n",
    "\n",
    "<div class='exercise'> \n",
    "    \n",
    "**[1.5.1:](#q151)** **Load and create tokenizer for GPT-2**\n",
    "    \n",
    "Load `GPT-2` from `HuggingFace`'s libraries. Then, fine-tune it on our Harry Potter Book #1 (`Book_1_The_Philosophers_Stone.txt`).\n",
    "\n",
    "* Use `GPT2Tokenizer` to tokenize the input text\n",
    "* NOTE: In [GPT2Tokenizer](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer), use `distilgpt2` as the `vocab_file` argument.\n",
    "* Split the input text into into blocks of length **100**\n",
    "* Generate inputs and labels by by shifting the input by one position:\n",
    "    * For example if your tokens are [1,2,3,4,5] then your inputs will be: [1,2,3,4] and labels will be: [2,3,4,5]\n",
    " \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training text\n",
    "training_file = \"data/harry_potter/Book_1_The_Philosophers_Stone.txt\"\n",
    "with open(training_file) as file:\n",
    "    training_data = file.read()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "#### Your code here ####\n",
    "\n",
    "#### End code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"s152\"></a>\n",
    "\n",
    "<div class='exercise'> \n",
    "    \n",
    "**[1.5.2:](#q152)** **Prepare data for GPT-2**\n",
    "   \n",
    "* Create TF Datasets with your inputs and labels\n",
    "* Use a batch size of 12\n",
    "* When creating tf.data pipelines for training dataset. Follow this order when building the pipeline:\n",
    "  * Shuffle\n",
    "  * Batch\n",
    "  * Prefetch\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s153\"></a>\n",
    "\n",
    "<div class='exercise'> \n",
    "    \n",
    "**[1.5.3:](#q153)** **Finetune GPT-2**\n",
    "\n",
    "* Build a model using `TFGPT2LMHeadModel` from the `transformers` package from Hugging Face\n",
    "* Load the pre-trained weights using `distilgpt2`\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"s154\"></a>\n",
    "\n",
    "<div class='exercise'>\n",
    "    \n",
    "**[1.5.4:](#q154)** **GPT-2 - model training**\n",
    "    \n",
    "* Use the `Adam` optimizer with a `learning_rate = 3e-5`, epsilon = `1e-08` and clipnorm = `1.0`\n",
    "* Use the loss `SparseCategoricalCrossentropy(from_logits=True)`. The pretrained `distilgpt2` has **6** Transformer layers. When fine-tuning our language model, we do not want to compute losses for every layers. So, when defining the loss functions, remember to set `None` for all of the transformer output layers. We will need to pass in an _array_ of loss functions to the `model.compile(...)`, rather than just one.\n",
    "* Use the metrics `SparseCategoricalAccuracy('accuracy')`\n",
    "* Train the model for at least **30 epochs**. The more you train, the better it will be at generating high-quality text (as you will see in the next question).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s155\"></a>\n",
    "\n",
    "\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[1.5.5:](#q155)** **Generate text with GPT-2**\n",
    "\n",
    "Now, using your GPT-2 model, generate 5 sentences of Harry Potter text!\n",
    "    \n",
    "* Use a prompt (2-3 words) and tokenize the words\n",
    "* Use the `model.generate(...)` function to generate output text\n",
    "* Here an example text generated from the finetuned GPT2 model\n",
    "    * `hufflepuff hates the slytherins.” “but what about you slytherin they hate me.” “oh they hate me!” said harry. “well i’ve got to go — i’ve got to go hogwarts!” “oh you are not going to be allowed goin’ around hogwarts is that funny professor — you know you can’t afford to go off to the bloody baron`\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s156\"></a>\n",
    "\n",
    "<div class='exercise'>\n",
    "    \n",
    "**[1.5.6:](#q156)** **Interpret results**\n",
    "    \n",
    "Reflect on your GPT-2's output. Please write 3-5 sentences, in total (not per bulleted point), about:\n",
    "1. its semantic and syntactic quality compared to the NGram LM's output\n",
    "2. weaknesses and strengths you see with the generated text\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 2. Word Embeddings [15 pts]\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2intro\"></a>\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Return to contents](#contents)\n",
    "    \n",
    "As we saw in lecture, computers have no inherit way of making sense of any text. NLP, at large, concerns this very process of trying to intelligently \"understand\" and make use of language. The representation of language, especially at the word-level, is incredibly connected to a model's ability to perform well on any task. With a poor representation, models may never be able to use the text data.\n",
    "\n",
    "Here, in Part 2, you will gain more hands-on experience working with and understanding word embeddings -- specifically, we will work with distributed, contextualized embeddings, which have been the de facto standard in NLP since 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2questions\"></a>\n",
    "\n",
    "## PART 2: Questions\n",
    "The goal of this task train a model to create a word2vec embedding using text from Harry Potter Books.\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "<a id=\"q21\"></a>\n",
    "\n",
    "<hr style=\"height:1pt\">\n",
    "\n",
    "**[2.1:](#s21)** **Build Word2Vec Embeddings**\n",
    "\n",
    "<a id=\"q211\"></a>\n",
    "\n",
    "**[2.1.1:](#s211)** **Load and preprocess Data**\n",
    "    \n",
    "For this section we will use Harry Potter Book #1 (`Book_1_The_Philosophers_Stone.txt`). You are welcome to train on more Harry Potter books. The more data you train with, the better your word embeddings will be.\n",
    "\n",
    "* Load text from book 1 `Book_1_The_Philosophers_Stone.txt`\n",
    "* Tokenize text using `nltk`'s word tokenizer, removing all punctuations\n",
    "* Remove stopwords\n",
    "* Use the provide helper function, `hw_utils.build_dataset(words, vocab_size)`, to generate sequential data. Your vocabulary size is the number of unique words types.\n",
    "\n",
    "\n",
    "<a id=\"q212\"></a>\n",
    "\n",
    "**[2.1.2:](#s212)** **Create a Skip Grams dataset**\n",
    "\n",
    "* Use the `skipgrams` function from tensorflow.keras to create the training dataset\n",
    "* Use `window_size` of 4\n",
    "* The output from `skipgrams` will consist of a word couples and labels. Split the couples into a target and context\n",
    "    * For example if the outputs from `skipgrams` are `couples:[[4, 8], [966, 334], [87, 5326], [452, 4139], [744, 1988]] labels: [1, 1, 0, 0, 0]`\n",
    "    * We need our training data in this format `target: [4, 966, 87, 452, 744], context: [8, 334, 5326, 4139, 1988], labels: [1, 1, 0, 0, 0]`\n",
    "\n",
    "\n",
    "<a id=\"q213\"></a> \n",
    "\n",
    "**[2.1.3:](#s213)** **Create a Skip Grams dataset**\n",
    "\n",
    "* Create TF Datasets with your target, context, and labels\n",
    "* Use a batch size of 1024\n",
    "* When creating `tf.data` pipelines for training dataset. Follow this order when building the pipeline:\n",
    "  * Shuffle\n",
    "  * Batch\n",
    "  * Prefetch\n",
    "  \n",
    " \n",
    "<a id=\"q214\"></a>\n",
    "    \n",
    "**[2.1.4:](#s214)** **Build & Train a Word2Vec Skip Gram Model**\n",
    "\n",
    "* Build a Word2Vec with an embedding dimension of **128**\n",
    "* Your model would require a word embedding and context embedding\n",
    "* Take the output from the word embedding and context embedding and perform a dot product\n",
    "* Use `sigmoid` for your output activation\n",
    "* Use `Adam` optimizer with `learning_rate = 0.01`\n",
    "* Use `binary_crossentropy` as the loss function\n",
    "* Use `accuracy` as the metrics\n",
    "* Train for 10 or more epochs\n",
    "\n",
    "<a id=\"q22\"></a>\n",
    "\n",
    "<hr style=\"height:1pt\">\n",
    "\n",
    "**[2.2:](#s22)** **Analyze Word2Vec**\n",
    "\n",
    "<a id=\"q221\"></a>\n",
    "\n",
    "**[2.2.1:](#s221)** **Pairwise Similarity**\n",
    "\n",
    "Let us analyze the trained word2vec embeddings\n",
    "    \n",
    "* From the trained model get the weights from the `word embedding` layer\n",
    "* Find the top `5` words similar to these words `'hogwarts','quidditch','dumbledore','gryffindor','hermione','hagrid'`\n",
    "* Use the util function provided `find_similar_words(...)`  to find similar words\n",
    " \n",
    "\n",
    "<a id=\"q222\"></a>\n",
    "**[2.2.2:](#s222)** **Interpret Similarity**\n",
    "* Discuss your thoughts on what similar words means for these embeddings\n",
    "\n",
    "<a id=\"q223\"></a>\n",
    "\n",
    "**[2.2.3:](#s223)** **Visualize Embeddings**\n",
    "\n",
    "* Get a list of similar words and embeddings by passing the whole `word_list = ['hogwarts','quidditch','dumbledore','gryffindor','hermione','hagrid']` to the `find_similar_words(...)` function\n",
    "* Use the given plotting code to plot and visualize your embeddings\n",
    "\n",
    "<a id=\"q224\"></a>\n",
    "\n",
    "**[2.2.4:](#s224)** **Interpret Embeddings**\n",
    "\n",
    "* What can you interpret from the the embeddings plotted in the plot above? How might you make the embeddings better?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2solutions\"></a>\n",
    "\n",
    "## PART 2: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s21\"></a>\n",
    "\n",
    "### **[2.1:](#q21)** **Build Word2Vec Embeddings**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s211\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[2.1.1:](#s211)** **Load and preprocess Data**\n",
    "    \n",
    "For this section we will use Harry Potter Book #1 (`Book_1_The_Philosophers_Stone.txt`). You are welcome to train on more Harry Potter books. The more data you train with, the better your word embeddings will be.\n",
    "\n",
    "* Load text from book 1 `Book_1_The_Philosophers_Stone.txt`\n",
    "* Tokenize text using `nltk`'s word tokenizer, removing all punctuations\n",
    "* Remove stopwords\n",
    "* Use the provide helper function, `hw_utils.build_dataset(words, vocab_size)`, to generate sequential data. Your vocabulary size is the number of unique words types.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s212\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[2.1.2:](#q212)** **Create a Skip Grams dataset**\n",
    "\n",
    "* Use the `skipgrams` function from tensorflow.keras to create the training dataset\n",
    "* Use `window_size` of 4\n",
    "* The output from `skipgrams` will consist of a word couples and labels. Split the couples into a target and context\n",
    "    * For example if the outputs from `skipgrams` are `couples:[[4, 8], [966, 334], [87, 5326], [452, 4139], [744, 1988]] labels: [1, 1, 0, 0, 0]`\n",
    "    * We need our training data in this format `target: [4, 966, 87, 452, 744], context: [8, 334, 5326, 4139, 1988], labels: [1, 1, 0, 0, 0]`\n",
    "\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s213\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[2.1.3:](#q213)** **Create a Skip Grams dataset**\n",
    "\n",
    "* Create TF Datasets with your target, context, and labels\n",
    "* Use a batch size of 1024\n",
    "* When creating `tf.data` pipelines for training dataset. Follow this order when building the pipeline:\n",
    "  * Shuffle\n",
    "  * Batch\n",
    "  * Prefetch\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s214\"></a>\n",
    "<div class='exercise'>  \n",
    "    \n",
    "**[2.1.4:](#q214)** **Build & Train a Word2Vec Skip Gram Model**\n",
    "\n",
    "* Build a Word2Vec with an embedding dimension of **128**\n",
    "* Your model would require a word embedding and context embedding\n",
    "* Take the output from the word embedding and context embedding and perform a dot product\n",
    "* Use `sigmoid` for your output activation\n",
    "* Use `Adam` optimizer with `learning_rate = 0.01`\n",
    "* Use `binary_crossentropy` as the loss function\n",
    "* Use `accuracy` as the metrics\n",
    "* Train for 10 or more epochs\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s22\"></a>\n",
    "\n",
    "### **[2.2:](#q22)** **Analyze Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s221\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[2.2.1:](#q221)** **Pairwise Similarity**\n",
    "\n",
    "Let us analyze the trained word2vec embeddings\n",
    "    \n",
    "* From the trained model get the weights from the `word embedding` layer\n",
    "* Find the top `5` words similar to these words `'hogwarts','quidditch','dumbledore','gryffindor','hermione','hagrid'`\n",
    "* Use the util function provided `find_similar_words(...)`  to find similar words\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s222\"></a>\n",
    "<div class='exercise'>  \n",
    "    \n",
    "**[2.2.2:](#q222)** **Interpret Similarity**\n",
    "\n",
    "* Discuss your thoughts on what \"similar words\" means for these embeddings\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s223\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[2.2.3:](#q223)** **Visualize Embeddings**\n",
    "\n",
    "* Get a list of similar words and embeddings by passing the whole `word_list = ['hogwarts','quidditch','dumbledore','gryffindor','hermione','hagrid']` to the `find_similar_words(...)` function\n",
    "* Use the given plotting code to plot and visualize your embeddings\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s224\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[2.2.4:](#q224)** **Interpret Embeddings**\n",
    "\n",
    "* What can you interpret from the the embeddings plotted in the plot above? How might you make the embeddings better?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 3. Text Classification [35 pts]\n",
    "\n",
    "[Return to contents](#contents)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3intro\"></a>\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Return to contents](#contents)\n",
    "    \n",
    "Throughout CS109A and CS109B, we have often modelled classification tasks. There is an infinite number of classification tasks that one could perform with text data, as there is no limit to the contents (and goals) of language. In fact, NLP has several sub-fields/ popular problems that are largely treated as classification tasks (e.g., sentiment analysis, natural language entailment, and generic 'text classification' like spam detection). Moreover, _nearly all_ NLP problems have at least some classification component.\n",
    "\n",
    "Here, in Part 3, you will gain experience with text classification by working on an actual real-world task that Chris Tanner encountered this past year:\n",
    "\n",
    "Medical research is produced at an astronomical rate ([a few thousand articles are published daily](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3191655/)). Thus, when embarking on a particular research study, conducting a proper literature search can be unwieldy and overwhelming. One typically provides very carefully crafted search terms, then has to sift through several thousand results. A doctor reads the **Abstracts** of thousands of these candidate papers. This process is referred to as a **Systematic Review**.\n",
    "\n",
    "Often times, the **Systematic Review** only yields a handful of useful research papers. It's akin to \"searching for a needle in a haystack\". The research papers that aren't of interest are considered `irrelevant`, and the potentially useful ones are considered `not irrelevant` (meaning, it will be read further and potentially used in the study). Clearly, this process of classifying papers for a particular research topic requires having expert-level knowledge of the research and medical domain. It's really important to not overlook useful (non-irrelevant) papers. Oftentimes two doctors will read over the same exact list of thousands of abstracts, just to ensure no papers are overlooked.\n",
    "\n",
    "If the Systematic Review yields _many_ useful papers, then one might be able to conduct a **Meta Analysis**, allowing one to draw new insights and research conclusions from the myriad of independent, regionalized research through the world. So, one needs to be incredibly meticulous when reading through thousands of abstracts. Wouldn't it be great if NLP could assist in this task? Well, let's find out if it can!\n",
    "\n",
    "For Part 3, you will help implement a text classifier for the **Systematic Review** process. In this real-life situation, an infectious disease doctor is researching sexually transmitted infections (STIs) in women who have HIV and are living in sub-Saharan Africa. STIs like __gonorrhea__ and __chlamydia__ are under-treated in low-resource communities. Because there aren't affordable and accessible STI testing in the area, there isn't population-wide screen. So, doctors don't have a good understanding of the epidemiology and prevalence of STIs -- especially amongst women who have HIV, which carries extra, serious health risks.\n",
    "\n",
    "Let's build a text classifier to see if we can help find `not irrelevant` abstracts. We can train the model by providing many _already-annotated_ abstracts, where each abstract is labelled as being `irrelevant` or `not irrelevant`. At _test time_, let's see if your model can help \"suggest\" which papers to strongly consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3questions\"></a>\n",
    "\n",
    "## PART 3: Questions\n",
    "The goal of this task is to use medical abstracts from the conference paper dataset and build/train a model to classify whether paper abstracts are irrelevant or not.\n",
    "\n",
    "[Return to contents](#contents)\n",
    "<hr/>\n",
    "<a id=\"q31\"></a>\n",
    "\n",
    "#### **[3.1:](#s31)** **Load and preprocess Data**\n",
    "\n",
    "<a id=\"q311\"></a>\n",
    "\n",
    "**[3.1.1:](#s311)** \n",
    "\n",
    "Download the dataset (code provided below). The dataset consists of 3 files, which you'll need to load:\n",
    "- `review_78678_irrelevant.csv`\n",
    "- `review_78678_not_irrelevant_included.csv` and\n",
    "- `review_78678_not_irrelevant_excluded.csv`\n",
    "\n",
    "All of these three should be present inside `datasets/project_1/` \n",
    "\n",
    "<a id=\"q312\"></a>\n",
    "\n",
    "**[3.1.2:](#s312)** \n",
    "* All 3 files have the same number of columns.\n",
    "* **We will be using just the `Abstract` column from all the files.**\n",
    "* Load the data from all the files into 3 dataframes. Create a new column (in all 3 dataframes) called `target` and assign `0` to it for the data from file `review_78678_irrelevant.csv` and `1` for the other two files.\n",
    "\n",
    "<a id=\"q313\"></a>\n",
    "\n",
    "**[3.1.3:](#s313)** \n",
    "* Concatenate all the dataframes to one keep just the column `Abstract` and `target`. Apply `dropna()` on the dataframe.\n",
    "\n",
    "\n",
    "<a id=\"q32\"></a>\n",
    "<hr/> \n",
    "\n",
    "#### **[3.2:](#s32)** **Build Data pipelines**\n",
    "\n",
    "For this section we will be using the `tf.data` API to build a simple but efficient data pipeline. The `tf.data` API enables you to build complex input pipelines from simple to complex reusable pieces. [Reference](https://www.tensorflow.org/guide/data)\n",
    "\n",
    "<a id=\"q321\"></a>\n",
    "**[3.2.1:](#s321)** \n",
    "* Set the variables - VOCABULARY_SIZE to 15000, SEQUENCE_SIZE (pick appropriate value), and EMBEDDING_SIZE to 100. Use [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) class from keras to build a text vectorizer for the entire dataset (Abstracts). Use output sequence length as 256 and `standardize_text()` provided below. \n",
    "\n",
    "<a id=\"q321a\"></a>\n",
    "**[3.2.1a:](#s321a)** What parameter did you choose for sequence length ? Justify your choice. \n",
    "\n",
    "<a id=\"q322\"></a>\n",
    "**[3.2.2:](#s322)** \n",
    "* Generate vocabulary. Display vocabulary length. Remember to use adapt() with TextVectorizer.\n",
    "\n",
    "<a id=\"q323\"></a>\n",
    "**[3.2.3:](#s323)** \n",
    "* Split the dataset with a validation percent 20%. Use a batch size of 32. Use train and validation shuffle buffer size equal to the number of data points in train and validation sets respectively. \n",
    "\n",
    "<a id=\"q324\"></a>\n",
    "**[3.2.4:](#s324)** \n",
    "* Build tf.data pipelines for the training and validation dataset. Follow this order when building the pipeline:\n",
    "  * Shuffle (if necessary) \n",
    "  * Batch\n",
    "  * Map (Convert your text to vectors)\n",
    "  * Prefetch\n",
    "* We will use this data pipeline for Model FFNN and LSTM.\n",
    "\n",
    "<a id=\"q324b\"></a>\n",
    "**[3.2.4b:](#s324b)** Explain why you should shuffle the data and describe when you should not shuffle the data. \n",
    "\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<a id=\"q33\"></a>\n",
    "#### **[3.3:](#s33)** **Model 1: Feed Forward Neural Network**\n",
    "\n",
    "<a id=\"q331\"></a>\n",
    "**[3.3.1:](#s331)** \n",
    "* The first model will be a simple Feed Forward Neural Network. When building a model ensure to give it a unique name since we will need this for comparing our results later. The model needs to have an Embedding layer followed by a Flatten layer and one or more Dense layers. \n",
    "\n",
    "\n",
    "<a id=\"q332\"></a>\n",
    "**[3.3.2:](#s332)** \n",
    "* Display model summary. \n",
    "* Train the model with the a `learning_rate = 0.003` and `epochs = 10`\n",
    "* The metric we want to monitor is `accuracy`.\n",
    "* When calling the `model.fit(...)` function make sure to get the training results. This can be done like `training_results = model.fit(...)`\n",
    "\n",
    "<a id=\"q332a\"></a>\n",
    "**[3.3.2a:](#s332a)** Name two reasons why using one-hot encoded vectors instead of using  the Embedding layer is not the way to go.\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"q332b\"></a>\n",
    "**[3.3.2b:](#s332b)** Explain what the inputs and outputs of the `Embedding` layer are. Also comment on the dimension going in and what is coming out.\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"q333\"></a>\n",
    "**[3.3.3:](#s333)** \n",
    "* Pass the training results to the `evaluate_save_model(model, validation_data, training_results, execution_time, learning_rate, epochs)`. This util function will plot your training history and save the model and metrics so we can compare all the models at the end.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "\n",
    "<a id=\"q34\"></a>\n",
    "### **[3.4:](#s34)** **Model 2: LSTM**\n",
    "\n",
    "The next model will be a Bi directional LSTM.  When building a model ensure to give it a unique name since we will need this for comparing our results later. \n",
    "\n",
    "<a id=\"q341\"></a>\n",
    "**[3.4.1:](#s341)** \n",
    "* Add an `Embedding` layer in your model\n",
    "* The Embedding layer is followed by a Bidirectional LSTM\n",
    "* The Bidirectional LSTM layer (with 64 units) can be followed by a few Dense layers\n",
    "\n",
    "\n",
    "<a id=\"q342\"></a>\n",
    "**[3.4.2:](#s342)** \n",
    "* Display model summary.\n",
    "* Train the model with the a `learning_rate = 1e-4` and `epochs = 10`\n",
    "* The metric we want to monitor is `accuracy` \n",
    "* When calling the `model.fit(...)` function make sure to get the training results. This can be done like `training_results = model.fit(...)`\n",
    "\n",
    "\n",
    "<a id=\"q342a\"></a>\n",
    "**[3.4.2a:](#s342a)** How many parameters does Bidirectional layer have ? (Show us how did you calculate)\n",
    "\n",
    "<a id=\"q343\"></a>\n",
    "**[3.4.3:](#s343)** \n",
    "* Pass the training results to the `evaluate_save_model(model, validation_data, training_results, execution_time, learning_rate, epochs)`. This util function will plot your training history and save the model and metrics so we can compare all the models at the end.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<a id=\"q35\"></a>\n",
    "\n",
    "### **[3.5:](#s35)** **Build Data Pipelines for BERT**\n",
    "\n",
    "BERT requires the data to be tokenized in a specific way, for this you need to use the `BertTokenizer` from the `transformers` package from Hugging Face. Steps to prepare your dataset:\n",
    "\n",
    "<a id=\"q351\"></a>\n",
    "**[3.5.1:](#s351)** \n",
    "\n",
    "* Use `BertTokenizer` to tokenize the input text\n",
    "* [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer), use `bert-base-uncased` as the `vocab_file` argument. Also set `do_lower_case=True`\n",
    "- When using `tokenizer.encode_plus(...)` set the `max_length` to a value suitable for the dataset. It would need to be `<=512`.\n",
    "- The output tokens from `tokenizer.encode_plus(...)` is a dictionary with the keys `'input_ids', 'token_type_ids', 'attention_mask'`\n",
    "\n",
    "<a id=\"q352\"></a>\n",
    "**[3.5.2:](#s352)**\n",
    "- Create TF Datasets using the tokenized results. When using `tf.data.Dataset.from_tensor_slices(...,...)` look out for the x values passed in. `BERT` requires 3 inputs `'input_ids', 'token_type_ids', 'attention_mask'` as a tuple.\n",
    "* Use a batch size of 32\n",
    "* Use train and validation shuffle buffer size equal to the number of data points in train and validation sets respectively\n",
    "* When creating tf.data pipelines for the training and validation dataset. Follow this order when building the pipeline:\n",
    "  * Shuffle (if necessary)\n",
    "  * Batch\n",
    "  * Prefetch\n",
    "\n",
    " \n",
    "<hr/>\n",
    "\n",
    "<a id=\"q36\"></a>\n",
    "### **[3.6:](#s36)** **Model 3: Finetune BERT**\n",
    "\n",
    "In this section we will finetune BERT for Sequence Classification\n",
    "\n",
    "<a id=\"q361\"></a>\n",
    "**[3.6.1:](#s361)** \n",
    "\n",
    "* Build a model using `TFBertForSequenceClassification` from the `transformers` package from Hugging Face\n",
    "* Load the pre-trained weights using `bert-base-uncased` make sure to set the `name` argument to a unique name since we will need this for comparing our results later\n",
    "\n",
    "<a id=\"q362\"></a>\n",
    "**[3.6.2:](#s362)** \n",
    "\n",
    "* Train the model with the a `learning_rate = 2e-5` and `epochs = 5`\n",
    "* The metrics we want to monitor is `accuracy`\n",
    "* When calling the `model.fit(...)` function make sure to get the training results. This can be done like `training_results = model.fit(...)`\n",
    "* Pass the training results to the `evaluate_save_model(model, validation_data, training_results, execution_time, learning_rate, epochs)`. This util function will plot your training history and save the model and metrics so we can compare all the models at the end.\n",
    "\n",
    "\n",
    "<a id=\"q362a\"></a>\n",
    "**[3.6.2a:](#s362a)** \n",
    "* How is BERT related (e.g., similar and different from) to the first Transformer (encoder + decoder) model that was introduced in the Attention is All you Need paper?\n",
    "\n",
    "\n",
    "<a id=\"q362b\"></a>\n",
    "**[3.6.2b:](#s362b)** What is the difference between attention and self-attention in the context of Transformer architecture?\n",
    "\n",
    " \n",
    "<hr/>\n",
    "\n",
    " <a id=\"q37\"></a>\n",
    "### **[3.7:](#s37)** **Compare all Models**\n",
    "\n",
    " \n",
    "<a id=\"q371\"></a>\n",
    "**[3.7.1:](#s371)**\n",
    "\n",
    " Do you think accuracy is a good metric here, why or why not ? If we were to incorporate f1_score metric with model training - how would you go about it ? Create a confusion matrix and also report and interpret precision, recall. Interpret the best F1 score reported with respect to the problem at hand. Which one is most important metric for researcher - precision or recall or F1-score (and why)?\n",
    " \n",
    " \n",
    " <hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3solutions\"></a>\n",
    "\n",
    "## PART 3: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s31\"></a>\n",
    "\n",
    "### **[3.1:](#q31)** **Load and preprocess Data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s311\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.1.1:](#q311)** Download the dataset (code provided below). The dataset consists of 3 files, which you'll need to load: `review_78678_irrelevant.csv`, `review_78678_not_irrelevant_included.csv`, and `review_78678_not_irrelevant_excluded.csv` all of which should be present inside `datasets/project_1/` \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download execution time (mins) 0.0038926124572753905\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to load all the data required for this part\n",
    "DATA_DIR = \"datasets\"\n",
    "start_time = time.time()\n",
    "hw_utils.download_file(\" https://cs109b-course-data.s3.amazonaws.com/project_1.zip\", base_path=\"datasets\", extract=True)\n",
    "execution_time = (time.time() - start_time)/60.0\n",
    "print(\"Download execution time (mins)\",execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s312\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.1.2:](#q312)** All 3 files have the same number of columns. We will be using just the **Abstract** column from all the files. Load the data from all the files into 3 dataframes. Create a new column (in all 3 dataframes) called target and assign 0 to it for the data from file `review_78678_irrelevant.csv` and 1 for the other two files.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s313\"></a>\n",
    "\n",
    "<div class='exercise'>\n",
    "    \n",
    "**[3.1.3:](#q313)**  Concatenate all the dataframes into a single dataframe. Keep only the columns **Abstract** and **target**. Apply dropna() on the dataframe.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s32\"></a>\n",
    "\n",
    "### **[3.2:](#q32)** **Build Data Pipelines**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"s321\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.2.1:](#q321)**\n",
    "\n",
    "Set these variables as follow:\n",
    "\n",
    "* VOCABULARY_SIZE to 15000\n",
    "* SEQUENCE_SIZE (pick appropriate value), and\n",
    "* EMBEDDING_SIZE to 100.\n",
    "\n",
    "Use the `TextVectorization` class from Keras to build a text vectorizer for the entire dataset (Abstracts). Use an output sequence length of **256**, along with the `standardize_text()` function provided below.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize text util function\n",
    "def standardize_text(input_text):\n",
    "  # Convert to lowercase\n",
    "  lowercase = tf.strings.lower(input_text)\n",
    "  # Remove HTML tags\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "  return tf.strings.regex_replace(\n",
    "      stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s321a\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.2.1a:](#q321a)** What value did you choose for the _output_sequence_length_? Justify your choice. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n",
    "Check length of words for abstracts, find median len words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s322\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.2.2:](#q322)** Use `TextVectorizer`'s `adapt()` method to generate vocabulary. Display vocabulary length.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s323\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.2.3:](#q323)**\n",
    "\n",
    "* Use `train_test_split()` to split the dataset, creating a validation set of size 20%.\n",
    "* Create TF Dataset. (**HINT:** use `tf.data.Dataset.from_tensor_slices`) \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"s324\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.2.4:](#q324)** Build `tf.data` pipelines for the training and validation dataset. Follow this order when building the pipeline:\n",
    "  * Shuffle your training data by using a buffer size that's equal to the number of training data points.\n",
    "  * Use a batch size of **32.**\n",
    "  * Map (Convert your text to vectors)\n",
    "  * `prefetch()` your validation set while specifying a buffer size \n",
    "\n",
    "We will use this data pipeline for Model FFNN and LSTM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"s324b\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.2.4b:](#q324b)** Explain why you should shuffle the data, and describe when you should **not** shuffle the data. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your answer here **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s33\"></a>\n",
    "\n",
    "### **[3.3:](#q33)** **MODEL 1: Feed Forward Neural Network**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s331\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.3.1:](#q331)** [WE PROVIDE; 0 pts]\n",
    "* The first model will be a simple Feed Forward Neural Network. When building a model, be sure to give it a unique name since we will need this for later comparing our results. The model needs to have an Embedding layer followed by a Flatten layer and one or more Dense layers. \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free code\n",
    "def build_ffnn():\n",
    "    \n",
    "  # Set the model name as\n",
    "  model_name = 'ffnn_'+str(int(time.time()))\n",
    "\n",
    "  # Create a FFNN Model\n",
    "  model = tf.keras.models.Sequential(name=model_name)\n",
    "  model.add(tf.keras.Input(shape=(SEQUENCE_SIZE)))\n",
    "  model.add(tf.keras.layers.Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_SIZE))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s332\"></a>\n",
    "<div class='exercise'>\n",
    "\n",
    "**[3.3.2:](#s332)** [WE PROVIDE; 0 pts]\n",
    "* Display model summary. \n",
    "* Train the model with the a `learning_rate = 0.003` and `epochs = 10`\n",
    "* The metric we want to monitor is `accuracy`.\n",
    "* When calling the `model.fit(...)` function make sure to get the training results. This can be done like `training_results = model.fit(...)`\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Training Params\n",
    "############################\n",
    "learning_rate = 0.003\n",
    "epochs = 10\n",
    "\n",
    "# Free up memory\n",
    "K.clear_session()\n",
    "\n",
    "# Build the model\n",
    "model = build_ffnn()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "# Optimizer\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "# Loss\n",
    "loss = keras.losses.binary_crossentropy\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "training_results = model.fit(\n",
    "        train_data,\n",
    "        validation_data=validation_data,\n",
    "        epochs=epochs, \n",
    "        verbose=1)\n",
    "execution_time = (time.time() - start_time)/60.0\n",
    "print(\"Training execution time (mins)\",execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s332a\"></a>\n",
    "<div class='exercise'>\n",
    "\n",
    "**[3.3.2a:](#q332a)** Name two reasons why one should use an **Embedding layer** instead of **one-hot encoded vectors**.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s332b\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.3.2b:](#q332b)** Explain what the inputs and outputs of the `Embedding` layer are. Also comment on the dimension going in and what is coming out\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s333\"></a>\n",
    "<div class='exercise'>  \n",
    "    \n",
    "**[3.3.3:](#q333)** \n",
    "* Pass the training results to the `evaluate_save_model(model, validation_data, training_results, execution_time, learning_rate, epochs)`. This util function will plot your training history and save the model and metrics so we can compare all the models at the end.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_utils.evaluate_save_model(model, validation_data, training_results, execution_time, learning_rate, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s34\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "### **[3.4:](#q34)** **MODEL 2: LSTM**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s341\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.4.1:](#s341)** \n",
    "* Add an `Embedding` layer in your model\n",
    "* Next, add a `Bidirectional` LSTM layer with 64 units\n",
    "* Next, add at least one `Dense` layers\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s342\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.4.2:](#s342)** \n",
    "* Display model summary.\n",
    "* Train the model with the a `learning_rate = 1e-4` and `epochs = 10`\n",
    "* The metric we want to monitor is `accuracy` \n",
    "* When calling the `model.fit(...)` function make sure to get the training results. This can be done like `training_results = model.fit(...)`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s342a\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "##### **[3.4.2a:](#q342a)**\n",
    "\n",
    "How many parameters does the `Bidirectional` layer have ? Show us your calculations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s343\"></a>\n",
    "<div class='exercise'>  \n",
    "    \n",
    "**[3.4.3:](#s343)** \n",
    "\n",
    "Pass the training results to the `evaluate_save_model(model, validation_data, training_results, execution_time, learning_rate, epochs)`. This util function will plot your training history and save the model and metrics so we can compare all the models at the end.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s35\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "### **[3.5:](#q35)** **Build Data Pipelines for BERT**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s351\"></a>\n",
    "<div class='exercise'> \n",
    "    \n",
    "**[3.5.1:](#q351)** \n",
    "\n",
    "* Use `BertTokenizer` to tokenize the input text\n",
    "* In the [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer), use `bert-base-uncased` as the `vocab_file` argument. Also set `do_lower_case=True`\n",
    "- When using `tokenizer.encode_plus(...)` set the `max_length` to a value suitable for the dataset. It would need to be `<=512`.\n",
    "- The output tokens from `tokenizer.encode_plus(...)` is a dictionary with the keys `'input_ids', 'token_type_ids', 'attention_mask'`\n",
    "                                                                                                                       \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s352\"></a>\n",
    "<div class='exercise'> \n",
    "    \n",
    "**[3.5.2:](#q352)**\n",
    "- Create TF Datasets using the tokenized results. When using `tf.data.Dataset.from_tensor_slices(...,...)` look out for the x values passed in. `BERT` requires 3 inputs `'input_ids', 'token_type_ids', 'attention_mask'` as a tuple.\n",
    "* Use a batch size of **32**\n",
    "* Use train and validation shuffle buffer size equal to the number of data points in train and validation sets respectively\n",
    "* When creating tf.data pipelines for the training and validation dataset. Follow this order when building the pipeline:\n",
    "  * Shuffle (if necessary)\n",
    "  * Batch\n",
    "  * Prefetch\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s36\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "### **[3.6:](#q36)** **MODEL 3: Finetune BERT**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"s361\"></a>\n",
    "\n",
    "<div class='exercise'> \n",
    "\n",
    "**[3.6.1:](#q361)** \n",
    "\n",
    "* Build a model using `TFBertForSequenceClassification` from the `transformers` package from HuggingFace\n",
    "* Load the pre-trained weights using `bert-base-uncased` make sure to set the `name` argument to a unique name since we will need this for comparing our results later\n",
    "\n",
    "</div>    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s362\"></a>\n",
    "\n",
    "<div class='exercise'> \n",
    "    \n",
    "**[3.6.2:](#q362)** \n",
    "    \n",
    "    \n",
    "* Train the model with the a `learning_rate = 2e-5` and `epochs = 5`\n",
    "* The metrics we want to monitor is `accuracy`\n",
    "* When calling the `model.fit(...)` function make sure to get the training results. This can be done like `training_results = model.fit(...)`\n",
    "* Pass the training results to the `evaluate_save_model(model, validation_data, training_results, execution_time, learning_rate, epochs)`. This util function will plot your training history and save the model and metrics so we can compare all the models at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s362a\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "##### **[3.6.2a:](#q362a)**\n",
    "\n",
    "How is BERT related (e.g., similar and different from) to the first Transformer (encoder + decoder) model that was introduced in the Attention is All you Need paper?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s362b\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "##### **[3.6.2b:](#q362b)**\n",
    "\n",
    "What is the difference between attention and self-attention in the context of Transformer architecture?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s37\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "### **[3.7:](#q37)** **Compare all Models**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to compare all models \n",
    "\n",
    "models_store_path = \"models\"\n",
    "\n",
    "models_metrics_list = glob(models_store_path+\"/*_metrics.json\")\n",
    "\n",
    "all_models_metrics = []\n",
    "for mm_file in models_metrics_list:\n",
    "  with open(mm_file) as json_file:\n",
    "    model_metrics = json.load(json_file)\n",
    "    all_models_metrics.append(model_metrics)\n",
    "\n",
    "# Load metrics to dataframe\n",
    "view_metrics = pd.DataFrame(data=all_models_metrics)\n",
    "\n",
    "# Format columns\n",
    "view_metrics['accuracy'] = view_metrics['accuracy']*100\n",
    "view_metrics['accuracy'] = view_metrics['accuracy'].map('{:,.2f}%'.format)\n",
    "\n",
    "view_metrics['trainable_parameters'] = view_metrics['trainable_parameters'].map('{:,.0f}'.format)\n",
    "view_metrics['execution_time'] = view_metrics['execution_time'].map('{:,.2f} mins'.format)\n",
    "view_metrics['loss'] = view_metrics['loss'].map('{:,.2f}'.format)\n",
    "view_metrics['f1_score'] = view_metrics['f1_score'].map('{:,.2f}'.format)\n",
    "view_metrics['model_size'] = view_metrics['model_size']/1000000\n",
    "view_metrics['model_size'] = view_metrics['model_size'].map('{:,.0f} MB'.format)\n",
    "\n",
    "# Filter columns\n",
    "view_metrics = view_metrics[[\"trainable_parameters\",\"execution_time\",\"loss\",\"accuracy\",\"f1_score\",\"model_size\",\"learning_rate\",\"epochs\",\"name\"]]\n",
    "\n",
    "view_metrics = view_metrics.sort_values(by=['f1_score'],ascending=False)\n",
    "view_metrics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"s371\"></a>\n",
    "<div class='exercise'>  \n",
    "\n",
    "**[3.7.1:](#q371)**\n",
    "\n",
    "* Do you think accuracy is a good metric here, why or why not?\n",
    "* Create a \"confusion matrix\" that simply includes the False Positive, False Negative, True Positive, and True Negative counts.\n",
    "* Also report and interpret Precision and Recall.\n",
    "* Interpret the best F1 score reported with respect to the problem at hand.\n",
    "* Which one is most important metric for the doctor (aka researcher): precision, recall, or F1-score (and why)? HINT: see the problem description for Part 3.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
